# 论文阅读部分

https://arxiv.org/pdf/2405.09980  - 金融领域数据集
[📎Chen 等 - 2024 - FinTextQA A dataset for long-form financial question answering.pdf](https://www.yuque.com/attachments/yuque/0/2025/pdf/2807993/1753323230917-3c900cf5-2ce7-4829-b813-419a84db0d5e.pdf)

## 关键词

长文本问答、金融领域

## 概述

论文**提出并开源了第一个金融领域的长文本问答基准数据集 FinTextQA**，然后用它**系统比较了 近似12 种 RAG 模块组合（4 个 Embedder × 3 个 Retriever × 3 个 Reranker × 4 个 Generator）**，得出“Ada2+AMR+Bge-Reranker+Baichuan2-7B”这一最佳配置，为今后任何金融/保险/监管类 LFQA 研究提供了“数据+实验范式”。

| 模块          | 代号                  | 本质 & 作用                                                  |
| ------------- | --------------------- | ------------------------------------------------------------ |
| **Embedder**  | **Ada2**              | **OpenAI 的 text-embedding-ada-002** 模型，把文本（问题或文档）转成 1536 维向量，用于第一层语义检索。 |
| **Retriever** | **AMR**               | **Auto-Merging Retriever**（LlamaIndex 实现）先把文档切成多级小片段（2048→512→128 token），如果多个子片段都命中，就自动把它们合并成更大的上下文，减少碎片化。 |
| **Reranker**  | **Bge-Reranker-Base** | **BAAI 的交叉编码器重排模型**，对上一步返回的候选片段再做一次“问题-片段”相关性打分，把最相关的排在前面，提高信噪比。 |
| **Generator** | **Baichuan2-7B**      | **百川智能开源的 70 亿参数大模型**，负责把检索到的证据 + 问题拼成提示，生成长答案。论文显示它在金融 QA 上与 GPT-3.5-turbo 准确度相当，但成本更低。 |



## 研究背景

​	金融领域**缺乏专门的长文本问答数据集**。现有金融问答基准存在不足，如**问题复杂度和多样性不够**，或只针对特定任务，无法满足实际长文本问答场景需求。因此作者引入FinTextQA数据集，以评估金融问答模型在一般金融及政策法规相关问题上的能力。

## 设计实验

### 数据集

论文里的 **1 262 条 QA 对** 全部来自 **人工筛选** 的原始材料（金融教科书和政府监管文件）。

流程：

1. 从教材/法规里 **人工摘录** 问题和参考答案；  
2. 人工再去找 **对应的原文段落** 作为证据；  
3. 经过两轮人工打分去掉低质量样本 → 得到 FinTextQA。  

### RAG

RAG部分的实验设置环境不同的模块使用了不同的工具

变量控制：固定其他模块，测试单一模块

噪声测试：逐渐增加输入文档的数量，观察：

- **GPT-4 综合评分**（有用性、准确性、深度）

  - **角色**：作为“智能评委”，替代人工评估答案质量。

  - **输入**：问题 + 检索到的证据 + 模型生成的答案。
    - **输出**：三个1-5分评分：
      1. **总分**：综合有用性、准确性、创造性。
      2. **问题-证据相关性**：是否切题。
      3. **证据-答案一致性**：答案是否基于证据。


  - **优势**：解决传统ROUGE/BLEU在长文本中高估的问题，更贴近人类判断。


- **ROUGE/BLEU** 与参考答案的匹配度

#### 测试RAG系统的实验设计

---

### **Step 1　单模块逐个体检**  
**目的**：找出每个环节里谁最强  

|     步骤     | 内容                                             | 举例（测 Embedder 时）                                       |
| :----------: | ------------------------------------------------ | ------------------------------------------------------------ |
| **1 选基线** | 把其余三个模块锁成\*\*“不差也不好”\*\*的中等模型 | Retriever = Vector<br>Reranker = All-Mpnet-base-v2<br>Generator = GPT-3.5-turbo |
| **2 换模型** | 只改动**被测模块**里的模型                       | Embedder 依次换成 Ada2 / Ember / Bge-small / Gte-Large       |
| **3 跑全集** | 用 FinTextQA **验证集** 全部 140+ 个问题跑一次   | 每换 1 个 Embedder 就跑 140+ 次                              |
|  **4 打分**  | GPT-4 对“问题 vs 检索回来片段”相关性 1-5 分      | 取平均分 → 得到 Ada2=4.586，Gte-Large=4.261                  |

| 被测模块      | 候选模型举例                   | 评价指标         | 关键结论                    |
| ------------- | ------------------------------ | ---------------- | --------------------------- |
| **Embedder**  | Ada2 / Bge-small / Gte-large … | GPT-4 相关性分数 | **Ada2 最高 4.586**         |
| **Retriever** | AMR / SWR / Vector             | GPT-4 相关性分数 | **AMR 最高 4.492**          |
| **Reranker**  | Bge-Reranker / LLMRerank …     | GPT-4 相关性分数 | **Bge-Reranker 最高 4.489** |

---

### **Step 2　黄金组合 A/B 测试**  
- **设计**：把 3×3×3×4 ≈ 100 种组合全部跑一次  
- **筛选规则**：只保留 GPT-4 分数最高的 **Top-3 组合**  
- **结果**：  
  1) `Ada2 + AMR + Bge-Reranker + Baichuan2-7B`  
  2) `Ada2 + AMR + LLMRerank + GPT-3.5-turbo`  
  3) `Ada2 + SWR + Bge-Reranker + GPT-3.5-turbo`

---

### **Step 3　鲁棒性（噪声）测试**  
**目的**：验证“文档越多、噪声越大”时系统是否崩  

- **变量**：输入文档数 n = 1 → 2 → 3  
- **观察**：  
  - 所有系统分数随 n 增加**缓慢下降**  
  - 当输入给 RAG 系统的上下文总长度 ≈ 34 k 后，再增加文档分数**几乎不变**  
- **结论**：Top-3 组合在长上下文场景下依然可用



## 研究结论、不足与展望

- **研究结论**：FinTextQA数据集为金融长文本问答提供了全面资源。在金融长文本问答任务中，Ada2、AMR、Bge - Reranker - Base和Baichuan2 - 7B的组合效果最佳，部分模型生成答案受人工青睐。
- **研究的创新性**：引入首个金融长文本问答数据集FinTextQA，将金融法规政策融入长文本问答，采用多方面评估方法和RAG框架评估模型。
- **研究的不足之处**：FinTextQA数据集的问答对数量相对大型人工智能生成数据集较少，可能影响模型在实际应用中的泛化能力，且高质量数据获取难、存在版权限制。
- **研究展望**：未来可进行数据增强，纳入更多样数据源扩展数据集，探索先进RAG能力和检索框架。

# 	思考部分

## QA对

论文中的QA对问题设计是复合问题，每个主问题包含几个子问题

![image-20250727222222026](./%E7%AC%AC%E4%B8%80%E5%91%A8-%E9%98%85%E8%AF%BB%E6%96%87%E7%8C%AE.assets/image-20250727222222026-1753626142983-3.png)

## RAG

实验设置环境不同的模块使用了不同的工具

嵌入器使用的四种流行模型

检索方法使用了基于向量、句子窗口检索(SWR)、自动合并检索（AMR）

AMR需要将保险合同分为**章节→条款→句子**三级，分别向量化

| **检索方法** | **是否需要向量化** | **如何应用**                                     |
| ------------ | ------------------ | ------------------------------------------------ |
| **AMR**      | ✅ 需向量化子节点   | 先用向量检索候选子节点，再通过树结构合并父节点。 |
| **SWR**      | ✅ 需向量化句子     | 先用向量匹配关键句子，再扩展固定窗口上下文。     |
| **传统向量** | ✅ 直接向量化文档块 | 无后续优化，依赖向量相似度。                     |

重排序使用了三种方案，如下

最后使用6种LLM进行打分

![image-20250727225644078](./%E7%AC%AC%E4%B8%80%E5%91%A8-%E9%98%85%E8%AF%BB%E6%96%87%E7%8C%AE.assets/image-20250727225644078-1753628204920-5.png)

![image-20250727225936247](./%E7%AC%AC%E4%B8%80%E5%91%A8-%E9%98%85%E8%AF%BB%E6%96%87%E7%8C%AE.assets/image-20250727225936247-1753628377200-7.png)





## 领域的相似性

论文讨论的是金融领域，项目关于保险领域，二者都符合下面的特征



![image-20250727220932954](./%E7%AC%AC%E4%B8%80%E5%91%A8-%E9%98%85%E8%AF%BB%E6%96%87%E7%8C%AE.assets/image-20250727220932954-1753625373918-1.png)



论文：

[ActuaryGPT：大语言模型在保险和精算工作中的应用 |英国精算杂志 |剑桥核心](https://www.cambridge.org/core/journals/british-actuarial-journal/article/actuarygpt-applications-of-large-language-models-to-insurance-and-actuarial-work/C99537965CCC826BEDD664044CC80A5A)

系统论述GPT类大模型在保险**条款解读、理赔问答、精算知识检索**中的应用，包含**RAG框架**设计、**长文本处理**技巧、**提示工程**模板，可直接对标FinTextQA的RAG实验部分。



