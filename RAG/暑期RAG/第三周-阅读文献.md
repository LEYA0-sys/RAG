# 上周的问题

## 7类简单的问题模板和3类复杂的问题模板

没有给出

## 不同类型的问题怎么调用不同的智能体

### 查询接收与初步处理

1. **用户提出查询**：
   - 用户向系统提出一个查询请求。

2. **子查询代理接收查询**：
   - 系统内部的子查询的Agent接收到用户的查询。

3. **评估查询质量**：
   - 子查询的Agent评估查询的质量，可能包括查询的清晰度、完整性等。

4. **分解为子查询**：
   - 将原始查询分解为更小、更具体的子查询，以便于处理。

5. **任务规划代理制定计划**：
   - 任务规划的Agent根据子查询制定一个初步的处理计划。

### 任务执行

6. **协调代理分配任务**：
   - 负责协调的Agent根据任务规划代理制定的计划，将任务分配给相应的专家代理。
7. **专家代理执行任务**：
   - 专家Agent执行分配给它们的任务，包括从数据库中检索数据、进行计算等。
8. **协调代理收集结果**：
   - 负责协调的Agent收集所有专家Agent执行任务后的结果。

## 怎么生成测试集

1. **定义问题模板**：
   - 论文中定义了7个简单问题模板和4个复杂问题模板，这些问题模板覆盖了不同的文件类型和查询场景。
2. **生成随机样本**：
   - 使用定义好的问题模板，生成随机样本。每个样本都确保有一个完整的答案，即采样来源是能够产生解决方案的有效输入的总空间。
3. **问题变体**：
   - 为了测试系统架构的鲁棒性，使用GPT-3.5-turbo作为查询重写器，对每个模板问题生成2个变体。这增加了问题的多样性和复杂性。
4. **标注路由**：
   - 每个问题都被标记为代理和表的适当路由。这意味着每个问题都附带了指示它应该由哪个专家代理处理的标签。
5. **执行规范解决方案**：
   - 对于每个问题，执行一个规范的解决方案来产生答案和所需的相关记录。这些记录用于后续的自动评估。
6. **数据集划分**：
   - 根据文件类型、难度和总体情况，将问题划分为不同的部分，以便进行更细致的评估。

## 不同类型文件的区别

| 文件类型 |      | 包含内容     | 提交频率 | 结构 | 完整性 | 可修改 |      |
| -------- | ---- | ------------ | -------- | ---- | ------ | ------ | ---- |
| 13F      |      | 持仓信息     | 季度     | XML  | 部分   | 是     |      |
| N-CSR    |      | 年度报告     | 年度     | 文本 | 多基金 | 是     |      |
| N-CEN    |      | 人口普查信息 | 年度     | XML  | 多基金 | 是     |      |
| N-PORT   |      | 持仓信息     | 季度     | XML  | 单基金 | 是     |      |
| N-MFP    |      | 持仓信息     | 月度     | XML  | 单基金 | 是     |      |
| ADV      |      | 实体信息     | 年度     | 文本 | 是     | 是     |      |

## 检索

论文只说明了使用嵌入（embeddings）和向量数据库（如FAISS库）来索引和检索数据，没有详细说明检索的过程

# **Engineering retrieval augmented generation-based virtual assistants in practice**

# 传统的VA

## 概念

**基于规则（rule-based）或决策树（decision-tree）** 的虚拟客服助手。人工预先写死所有问答路径，它完全依赖**人工预设的脚本、关键词匹配或有限状态机**，没有大模型的生成能力，也不做语义级检索

## 核心架构

```
┌────────────────────────┐
│ 1️⃣ 意图识别层           │
│  • 关键词/正则           │
│  • TF-IDF + 朴素贝叶斯   │
│  • SVM 分类             │
└──────────┬─────────────┘
           │
┌──────────┴─────────────┐
│ 2️⃣ 对话管理层           │
│  • 决策树（固定树）      │
│  • 槽位填充（正则）      │
│  • 缺槽追问             │   “槽位填充”就是“把用户说的话里，关键信息挖出来填到表格里；如果挖不	 						到，就反复追问直到填上。”
└──────────┬─────────────┘
           │
┌──────────┴─────────────┐
│ 3️⃣ 响应/履行层          │
│  • 静态模板             │
│  • 调用后端 API         │
└──────────┬─────────────┘
           │
┌──────────┴─────────────┐
│ 4️⃣ 记忆层               │
│  • 会话级槽位缓存        │
│  • 会话结束即清空        │
└────────────────────────┘
```



## 缺点

1. 难以帮助用户快速找到不常用业务（如一年一次的缴费）的准确信息；
2. 跨 Web/App 渠道的体验不一致，人工兜底能力有限；
3. 菜单式结构扩展困难，新增业务需大量手工改写规则；
4. 难维护，无法应对稍微偏离脚本的用户诉求

# RAG-VA

## 概念

RAG-VA 是 **Retrieval-Augmented Generation Virtual Assistant** 的缩写，直译为 **“检索增强生成虚拟助手”**。它是一个把两项核心技术融合在一起的智能客服概念：

1. **Retrieval（检索）**  
   先用向量检索或混合检索，从企业知识库、文档、FAQ、工单等大量资料里，实时找出与当前用户问题最相关的一段或几段信息。

2. **Generation（生成）**  
   再把检索到的内容连同用户问题一起送进大语言模型（LLM，如 GPT-4、Claude），由模型即时生成一段自然、准确、带上下文的回答。

## 区别

| 维度           | 传统 VA（规则/决策树型）                  | RAG-VA（检索增强生成）                   |
| -------------- | ----------------------------------------- | ---------------------------------------- |
| **技术核心**   | 关键词匹配 + 决策树/脚本                  | 向量检索 + 大模型生成                    |
| **知识来源**   | 人工预置的问答脚本或菜单                  | 动态检索外部文档/数据库                  |
| **回答方式**   | 固定模板，逐字不变                        | 大模型根据检索内容**实时**生成           |
| **更新成本**   | 每新增/修改意图要重写脚本，需全量回归测试 | **只需更新或增删文档，系统自动生效**     |
| **开放域能力** | 只能回答树上已覆盖的问题，偏离即“转人工”  | **能处理未见过的问题**，只要文档里有答案 |
| **上下文记忆** | 一轮会话内仅记住槽位值，会话结束即清零    | **支持长上下文甚至跨会话记忆**           |
| **多轮对话**   | 必须预先画出所有分支，复杂度高            | 由大模型按上下文自由推进，无需穷举路径   |
| **可扩展性**   | 线性增加脚本 → 维护噩梦                   | 文档增长对系统复杂度几乎无感             |
| **用户体验**   | 菜单式、机械感强                          | 自然语言交流，接近人工客服               |
| **典型局限**   | 扩展困难、体验不一致、信息查找耗时        | 需额外做检索质量、幻觉、合规治理         |

## 应用领域

> [!NOTE]
>
> 代码未开源/论文的数据来源是 **定性访谈**，而非任何形式的“数据集”或实验日志

**交通行业收费客服领域**

• 行业：道路运营 / 交通收费
• 具体场景：澳大利亚 Linkt 品牌的 e-TAG 收费客服
• 业务问题：通行费查询、发票补缴、账户管理、防诈骗等

# **定性实证研究**

## 研究做的内容

1. 与 9 名 Transurban 工程师进行了两轮、共 6.5 小时的焦点小组访谈（有录音、转写、编码）。
2. 把团队 **真实落地 RAG-VA 的全过程**（需求→数据→检索→生成→评估→上线）整理成可复制的工程框架。
3. 基于访谈内容，归纳出 8 大工程挑战与 22 个未来研究问题和RAI 指标清单

## 实验流程

| 阶段                                                         | 子环节               | 关键活动 & 输出物                                            |
| ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |
| Step 1 数据摄取（Data Ingestion Workflow）                   | 1a. 文档收集         | 从客服网站、工单系统、政策 PDF、图片/视频等多模态源批量抓取原始资料。 |
|                                                              | 1b. 数据工程         | 清洗→去噪→打元数据标签（内容类别、时效、关键词）。           |
|                                                              | 1c. 分块（Chunking） | 按语义/结构切成 100-512 token 的“自包含”块；图片先 OCR/ASR 转文本。 |
|                                                              | 1d. 向量化           | 使用领域微调后的嵌入模型（如 BERT、OpenAI Embeddings）将块转为向量。 |
|                                                              | 1e. 索引入库         | 写入向量数据库（Pinecone、Weaviate、Milvus 等）并建立混合索引（ANN + BM25）。 |
| Step 2 响应生成（Response Generation Workflow）              | 2a. 用户输入         | 接受多渠道（Web、App、IVR）自然语言 query。                  |
|                                                              | 2b. 输入护栏         | 敏感信息脱敏、Prompt-Injection 检测、GDPR 合规检查。  确保用户输入的信息是安全的，不会对系统造成威胁或泄露敏感信息。 |
|                                                              | 2c. 语义检索         | 用同嵌入模型将 query 向量化，Top-k 召回最相关块。            |
|                                                              | 2d. Prompt 增强      | 将检索到的块+对话上下文拼成 Prompt，指导 LLM 生成答案。      |
|                                                              | 2e. LLM 生成         | 调用 GPT-4/Claude/Llama 等，生成带引用来源的回答。           |
|                                                              | 2f. 输出护栏         | 运行 LlamaGuard、NeMo Guardrails 等进行毒性、偏见、幻觉二次过滤。确保系统生成的回答是安全的，没有毒性、偏见或幻觉内容。 |
| Step 3 评估与持续改进（RAG Evaluation & Continuous Improvement） | 3a. 测试用例库       | 通用基准（Natural Questions 等）+ 领域“最后一英里”手工用例 + LLM 自动生成变体。 |
|                                                              | 3b. 三维评估         | ① 检索精度（MRR、Recall、Precision）；② 生成质量（BLEU/ROUGE/BERTScore）；③ RAI 指标。 |
|                                                              | 3c. 线上监控         | 埋点收集用户点赞/点踩、人工客服复核；自动 A/B 实验，把线上流量按 固定比例 切成两条并行链路，跑不同模型/知识库，实时比较业务 KPI。；异常答案触发回滚。 |
|                                                              | 3d. 持续迭代         | 每月按评估报告更新知识库、微调嵌入、调整护栏规则、升级 LLM 版本。 |

> [!IMPORTANT]
>
> | 指标       | 匹配方式        | 是否理解语义 | 对同义改写鲁棒性 | 人类相关性 |
> | ---------- | --------------- | ------------ | ---------------- | ---------- |
> | BLEU/ROUGE | n-gram 精确匹配 | 否           | 差               | 0.7 左右   |
> | BERTScore  | 向量语义匹配    | 是           | 好               | 0.9+       |
>
> BERTScore = “让 BERT 当裁判，看两句话在语义空间里有多近”，从而弥补 BLEU/ROUGE 只看字面形式的缺陷。



## 八大工程挑战

|      | 挑战名称              | 一句话痛点                                                   | 主要出现环节 |
| ---- | --------------------- | ------------------------------------------------------------ | ------------ |
| 1    | 多模态数据工程        | 文档、图片、表格、视频混杂，统一向量化难                     | 数据摄取     |
| 2    | 自适应安全护栏        | 在保护用户隐私（防止个人信息泄露）和防止恶意攻击（如注入攻击）之间找到平衡，现有的固定规则可能不够灵活。 | 输入/输出    |
| 3    | LLM 版本迁移与 RAGOps | 当大型语言模型（如GPT）更新换代时，如何平滑迁移并确保新旧版本的兼容性，同时缺少自动化的运维流程。 | 部署运维     |
| 4    | 相关性与简洁性平衡    | 生成的回答可能过于简短而缺乏细节，或者过于冗长而显得啰嗦，难以在信息量和简洁性之间找到平衡。 | 响应生成     |
| 5    | 自动化测试            | 缺乏标准答案（Ground-Truth）来验证生成的答案是否正确，难以进行大规模的自动化测试。 | 测试         |
| 6    | 系统级评估指标        | 现有的评估指标（如BLEU、ROUGE）可能不足以全面评价系统性能，需要更综合的指标来衡量检索和生成的效果。 | 评估         |
| 7    | 人类反馈闭环          | 如何从大量的用户反馈中自动提取有价值的信息，并将其转化为系统改进的需求。 | 持续改进     |
| 8    | 负责任 AI（RAI）框架  | 现有的检测工具可能无法全面评估模型在幻觉、偏见等方面的风险，尤其是在结合知识库上下文时。 | 合规治理     |

## 22 个未来研究问题

```markdown
Challenge 1 多模态数据工程
RQ1-1 如何统一文本/图像/表格等不同模态的嵌入？
RQ1-2 分块策略对检索精度的定量影响？
RQ1-3 动态索引更新时的延迟-精度权衡？
Challenge 2 自适应安全护栏
RQ2-1 如何动态学习新攻击模式并实时更新规则？
RQ2-2 上下文敏感的 PII 识别算法？
Challenge 3 LLM 版本迁移
RQ3-1 如何检测新 LLM 导致检索结果漂移？
RQ3-2 向后兼容的 RAGOps 流水线模板？
Challenge 4 相关-简洁平衡
RQ4-1 除温度外，top-k/top-p/重复惩罚的最优组合？
RQ4-2 用户分层（新手/熟手）对回答长度偏好的影响？
Challenge 5 自动化测试
RQ5-1 无 Ground-Truth 场景下的测试 Oracle 如何生成？
RQ5-2 如何自动生成覆盖边缘场景的测试用例？
Challenge 6 系统级评估指标
RQ6-1 检索-生成联合指标如何设计？
RQ6-2 噪声鲁棒性、负面拒绝、反事实鲁棒性怎么量化？
Challenge 7 人类反馈闭环
RQ7-1 大规模点赞/吐槽如何自动聚类成可执行需求？
RQ7-2 如何量化“一条反馈 → 系统改进”的闭环 ROI？
Challenge 8 RAI
RQ8-1 如何结合知识库上下文检测幻觉？
RQ8-2 如何动态评估答案偏见/毒性而不过度拒绝？
RQ8-3 跨语言场景的 RAI 指标一致性问题？
```

## 7 项 RAI 指标

作用：用来衡量基于大模型的虚拟助手在 **伦理、合规、可信** 方面表现的一套量化指标

| 指标                 | 含义                                     | 典型阈值示例    |
| -------------------- | ---------------------------------------- | --------------- |
| Hallucination        | 回答与来源事实不符的比例                 | < 3 %           |
| Faithfulness         | 回答忠实于检索内容的比例                 | > 0.9           |
| Bias                 | 针对敏感属性（性别、地域等）的不公平倾向 | 差异分数 < 0.05 |
| Toxicity             | 冒犯、辱骂、歧视性语言                   | 0 命中          |
| Contextual Precision | 所用上下文与问题的相关度                 | > 0.85          |
| Contextual Recall    | 回答覆盖来源中关键信息的比例             | > 0.8           |
| Contextual Relevancy | 整段回答的整体相关性                     | > 0.9           |

## 创新点

| 维度       | 具体创新（还没开始做）                                       | 与现有工作的差异（工程师做过的了）              |
| ---------- | ------------------------------------------------------------ | ----------------------------------------------- |
| 工程框架   | 首个面向客服场景的 RAG-VA 全流程工程框架（数据→检索→生成→评估→持续改进） | 以往研究聚焦算法，缺乏端到端落地流程            |
| 挑战体系   | 从真实工业实践系统梳理出 8 大 SE 挑战（多模态数据工程、自适应安全护栏、LLM 版本迁移、相关-简洁平衡、自动化测试、系统指标体系、人类反馈闭环、RAI 框架） | 文献多为单点技术问题，未见如此体系化归纳        |
| 评估方法   | 提出“三层评估”：通用基准+领域最后一英里+线上 A/B；给出可操作的指标组合（BLEU/ROUGE/METEOR + RAI 指标 + 客户满意度） | 传统 RAG 评估侧重检索精度，未覆盖运营级持续监控 |
| 实践指南   | 给出平台选型、分块策略、嵌入模型/向量库选择、测试用例自动生成与维护的决策指引 | 现有指南多为概念级，缺乏与真实业务耦合的细节    |
| 多模态处理 | 将票据、车牌图片、操作手册等异构数据统一纳入 RAG 管道，并讨论 OCR/ASR/多模态检索模型选型 | 大多数 RAG 研究仍停留在纯文本                   |

## 检验标准

**三维、三层、三循环** 的立体评估体系  
一、三维评估指标（WHAT 测什么）

1. 检索维度  
   • Recall/ MRR / Precision：衡量 Top-k 块里是否包含正确答案。  
   • Contextual Precision & Recall：答案所用上下文与真实需求的相关度。  
   • 噪声鲁棒性（Noise Robustness）：在检索到 20% 不相关块时，系统仍能给出正确回答的比例。

2. 生成维度  
   • BLEU / ROUGE / METEOR：与参考答案的 n-gram 重叠。  
   • BERTScore / G-EVAL：语义相似度，避免“同义不同字”被误判。  
   • 忠实度 Faithfulness：回答中事实与检索来源的一致性，防幻觉。  
   • 简洁-相关平衡：用“压缩率”（生成字数 / 检索字数）+ 用户满意度联合打分。

3. RAI维度  
   •  **伦理、合规、可信** 方面  
   • GDPR/AI-Act 合规检查：是否泄露个人可识别信息。

──────────────── ──────────────────────────── ──────────────── ────  
二、三层评估场景（WHEN & WHERE 测）

1. 通用基准测试（离线）  
   • 数据集：Natural Questions、BEIR、WikiEval。  
   • 作用：快速回归，验证模型升级后整体能力不退化。

2. 领域“最后一英里”测试（离线 + 人工）  
   • 由业务专家撰写 100–500 条高价值场景用例（如“用户想补缴 6 个月前的通行费”）。  
   • 关键：必须包含“无标准答案”的开放性问题，检验 RAG 的“创造性忠实”。

3. 线上真实场景监控（在线）  
   • 埋点：首次响应时长、会话解决率、转人工率、用户点赞/点踩。  

    **A 组 5%（旧模型/旧知识库）** + **B 组 95%（新模型/新知识库）**，观察业务 KPI

──────────────────────────────── ──────────────── ──────────────── ─  
三、三循环持续改进（HOW 迭代）

1. 自动化日循环  
   • 每晚跑 1000 条合成用例，触发红线即回滚。  
   • 工具：RAGAS + 自研脚本，输出 JSON 报告进 CI/CD。

2. 周循环人工复核  
   • 客服团队抽样 50 条高分/低分对话，标记错误类型 → 反哺知识库或护栏规则。  
   • 计算“修复-复测”周期 ≤ 5 个工作日。

3. 月循环战略评估  
   • 业务方 + 技术方评审：根据 ROI（客服成本节省 vs. 系统维护成本）决定  
     ① 是否升级 LLM；② 是否扩大 RAG 到新的业务线。

──────────────────────────────── ──────────────── ────────────────   
落地 Checklist

| 阶段           | 关键交付物                                    | 通过标准示例                 |
| -------------- | --------------------------------------------- | ---------------------------- |
| 离线通用基准   | 评估报告（Recall@5 ≥ 0.85, BERTScore ≥ 0.82） | 任一指标下降 > 3% 即阻断发版 |
| 领域最后一英里 | 业务用例覆盖率≥ 95%，人工评分≥ 4/5            | 专家盲测                     |
| 线上监控       | 仪表盘：7 日平均解决率 ≥ 75%，幻觉投诉 < 1%   | 实时监控告警                 |
