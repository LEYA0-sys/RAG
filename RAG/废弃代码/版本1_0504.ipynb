{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f319136",
   "metadata": {},
   "source": [
    "安装必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f62769f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script chroma.exe is installed in 'C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langchain langchain_community pypdf sentence_transformers chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95f28163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "[INFO] NumExpr defaulting to 8 threads.\n",
      "[INFO] PyTorch version 2.5.1 available.\n",
      "[INFO] Polars version 1.27.1 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain                     0.3.25\n",
      "langchain_community           0.3.23\n",
      "pypdf                         5.4.0\n",
      "sentence_transformers         4.1.0\n",
      "chromadb                      1.0.8\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库并打印版本信息\n",
    "import langchain, langchain_community, pypdf, sentence_transformers, chromadb\n",
    "\n",
    "for module in (langchain, langchain_community, pypdf, sentence_transformers, chromadb):\n",
    "    print(f\"{module.__name__:<30}{module.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3fb14bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入操作系统和数据处理相关库\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain_community.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df6d7081",
   "metadata": {},
   "outputs": [],
   "source": [
    "#嵌入模型选择ollama拉取的\"znbang/bge:large-zh-v1.5-q8_0\"\n",
    "EMBEDDING_MODEL_PATH = 'BAAI/bge-large-zh-v1.5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e26224ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#五月份第一次尝试的版本，输出物保存在该路径\n",
    "dt = '2025-05'\n",
    "version = 'v1'\n",
    "output_dir = os.path.join('outputs', f'{version}_{dt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bb3a02",
   "metadata": {},
   "source": [
    "文档切分\n",
    "\n",
    "先加载未切分过文档\n",
    "再把切分以后的文档放在output_dir下\n",
    "目前已经有了一个切分好的文档，位于\"split_data\\all_docs_split_400_40.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24807e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:82: SyntaxWarning: invalid escape sequence '\\R'\n",
      "<>:82: SyntaxWarning: invalid escape sequence '\\R'\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13972\\1838553486.py:82: SyntaxWarning: invalid escape sequence '\\R'\n",
      "  input_dir = \"E:\\RAG\\split_data\"  #已经处理过的文档的路径\n",
      "[INFO] Loaded 0 documents from E:\\RAG\\split_data\n",
      "[INFO] Splitting 0 documents...\n",
      "[INFO] Found existing cache. Loading...\n",
      "[INFO] Loaded 0 chunks from outputs\\v1_2025-05\\all_docs_split_400_40.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "import json\n",
    "import uuid\n",
    "import logging\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
    "\n",
    "@dataclass\n",
    "class SplitConfig:\n",
    "    chunk_size: int = 400\n",
    "    chunk_overlap: int = 40\n",
    "    separators: List[str] = ('\\n\\n\\n', '\\n\\n')\n",
    "    force_split: bool = False\n",
    "    output_format: str = 'json'\n",
    "    cache_dir: str = output_dir\n",
    "\n",
    "def save_chunks_as_json(chunks, filepath):\n",
    "    data = [\n",
    "        {\n",
    "            \"uuid\": chunk.metadata.get('uuid', str(uuid.uuid4())),\n",
    "            \"content\": chunk.page_content,\n",
    "            \"metadata\": chunk.metadata\n",
    "        }\n",
    "        for chunk in chunks\n",
    "    ]\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def load_chunks_from_json(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    chunks = []\n",
    "    for item in data:\n",
    "        doc = Document(\n",
    "            page_content=item['content'],\n",
    "            metadata=item['metadata']\n",
    "        )\n",
    "        chunks.append(doc)\n",
    "    logging.info(f\"Loaded {len(chunks)} chunks from {filepath}\")\n",
    "    return chunks\n",
    "\n",
    "def split_docs_with_config(documents, config: SplitConfig, cache_name=\"all_docs\"):\n",
    "    os.makedirs(config.cache_dir, exist_ok=True)\n",
    "    filename = f\"{cache_name}_split_{config.chunk_size}_{config.chunk_overlap}.{config.output_format}\"\n",
    "    filepath = os.path.join(config.cache_dir, filename)\n",
    "\n",
    "    if os.path.exists(filepath) and not config.force_split:\n",
    "        logging.info(\"Found existing cache. Loading...\")\n",
    "        return load_chunks_from_json(filepath)\n",
    "\n",
    "    splitter = MarkdownTextSplitter(\n",
    "        chunk_size=config.chunk_size,\n",
    "        chunk_overlap=config.chunk_overlap\n",
    "    )\n",
    "    chunks = splitter.split_documents(documents)\n",
    "    for chunk in chunks:\n",
    "        chunk.metadata['uuid'] = str(uuid.uuid4())\n",
    "\n",
    "    if config.output_format == 'json':\n",
    "        save_chunks_as_json(chunks, filepath)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def load_multiple_documents_from_dir(directory: str, encoding='utf-8') -> List[Document]:\n",
    "    docs = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".md\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            loader = TextLoader(file_path, encoding=encoding)\n",
    "            file_docs = loader.load()\n",
    "            for doc in file_docs:\n",
    "                doc.metadata['source_file'] = filename\n",
    "            docs.extend(file_docs)\n",
    "    logging.info(f\"Loaded {len(docs)} documents from {directory}\")\n",
    "    return docs\n",
    "\n",
    "input_dir = \"E:\\RAG\\split_data\"  #已经处理过的文档的路径\n",
    "documents = load_multiple_documents_from_dir(input_dir)\n",
    "\n",
    "split_config = SplitConfig(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=40,\n",
    "    separators=['\\n\\n\\n', '\\n\\n'],\n",
    "    force_split=False,\n",
    "    output_format='json',\n",
    "    cache_dir=output_dir\n",
    ")\n",
    "# 切分文档并保存到本地\n",
    "logging.info(f\"Splitting {len(documents)} documents...\")\n",
    "split_docs = split_docs_with_config(documents, split_config, cache_name=\"all_docs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df975b3c",
   "metadata": {},
   "source": [
    "向量化\n",
    "\n",
    "此部分代码是开源项目中的，后续会更改\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "255b1d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13972\\222456044.py:8: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceBgeEmbeddings(\n",
      "[INFO] Load pretrained SentenceTransformer: BAAI/bge-large-zh-v1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "# 初始化嵌入模型\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'device: {device}')\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_PATH,\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# 确保模型只加载一次\n",
    "if 'embeddings' not in globals():\n",
    "    embeddings = HuggingFaceBgeEmbeddings(\n",
    "        model_name=EMBEDDING_MODEL_PATH,\n",
    "        model_kwargs={'device': device},\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "    print(\"嵌入模型加载成功\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4dc6810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义向量数据库生成函数\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def get_vector_db(docs, store_path, force_rebuild=False):\n",
    "    if not os.path.exists(store_path):\n",
    "        force_rebuild = True\n",
    "\n",
    "    if force_rebuild:\n",
    "        vector_db = Chroma.from_documents(\n",
    "            docs,\n",
    "            embedding=embeddings,\n",
    "            persist_directory=store_path\n",
    "        )\n",
    "    else:\n",
    "        vector_db = Chroma(\n",
    "            persist_directory=store_path,\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "    return vector_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb04a8f",
   "metadata": {},
   "source": [
    "创建向量数据库\n",
    "\n",
    "区别：\n",
    "\n",
    "1.开源的数据库使用的是chromab\n",
    "2.实践中使用的是FAISS\n",
    "\n",
    "此处只是简单地修改了一下开源项目中的代码\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b970eae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13972\\2895368380.py:15: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_db = Chroma(\n",
      "[INFO] Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "# 创建向量数据库\n",
    "vector_db = get_vector_db(split_docs, store_path=os.path.join(output_dir, '向量数据库', 'bge_large_v1.5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c081a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载已经准备好的向量数据库\n",
    "from langchain.vectorstores import FAISS\n",
    "import pickle\n",
    "\n",
    "faiss_path = os.path.join('outputs', 'v1_2025-05', 'faiss_metadata.pkl')\n",
    "with open(faiss_path, 'rb') as f:\n",
    "    vector_db = pickle.load(f)\n",
    "print(\"向量数据库加载成功\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3d9912",
   "metadata": {},
   "source": [
    "检索\n",
    "\n",
    "检索部分进行了增强\n",
    "添加了混合检索器并且对问题进行了更改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e5508de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13972\\1662884068.py:16: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  results = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 检索结果 ===\n"
     ]
    }
   ],
   "source": [
    "# 替换混合检索器的实现\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 定义检索器，as_retriever方法用于创建检索器,k参数指定返回的文档数量\n",
    "retriever = vector_db.as_retriever(search_kwargs={'k': 5})\n",
    "\n",
    "# 自定义排序函数\n",
    "def rank_results(results, query):\n",
    "    # 示例：按相似性分数排序\n",
    "    return sorted(results, key=lambda x: x.metadata.get('score', 0), reverse=True)\n",
    "\n",
    "# 检索并排序\n",
    "query =\"如果购买了医疗保险，得重病时是否可以报销医疗费用？\"\n",
    "# 检索与query相关文档\n",
    "results = retriever.get_relevant_documents(query)\n",
    "# 对检索结果进行排序\n",
    "ranked_results = rank_results(results, query)\n",
    "\n",
    "# 格式化输出排序后的检索结果\n",
    "print(\"\\n=== 检索结果 ===\")\n",
    "for idx, result in enumerate(ranked_results, start=1):\n",
    "    print(f\"\\n===========结果{idx}==========:\")\n",
    "    print(f\"内容: {result.page_content}\")\n",
    "    print(f\"分数: {result.metadata.get('score', 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e1626cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义混合检索器类\n",
    "class HybridRetriever:\n",
    "    def __init__(self, retriever, keyword_weight=0.5, vector_weight=0.5):\n",
    "        self.retriever = retriever\n",
    "        self.keyword_weight = keyword_weight\n",
    "        self.vector_weight = vector_weight\n",
    "\n",
    "    def get_relevant_documents(self, query):\n",
    "        # 调用原始检索器获取文档\n",
    "        results = self.retriever.get_relevant_documents(query)\n",
    "        # 混合排序逻辑（示例：按权重调整分数）\n",
    "        for result in results:\n",
    "            result.metadata['hybrid_score'] = (\n",
    "                self.keyword_weight * result.metadata.get('keyword_score', 0) +\n",
    "                self.vector_weight * result.metadata.get('vector_score', 0)\n",
    "            )\n",
    "        return sorted(results, key=lambda x: x.metadata['hybrid_score'], reverse=True)\n",
    "\n",
    "# 初始化混合检索器\n",
    "hybrid_retriever = HybridRetriever(retriever, keyword_weight=0.6, vector_weight=0.4)\n",
    "\n",
    "# 示例：使用混合检索器\n",
    "query = \"如果购买了医疗保险，得重病时是否可以报销医疗费用？\"\n",
    "hybrid_results = hybrid_retriever.get_relevant_documents(query)\n",
    "for idx, result in enumerate(hybrid_results[:5], start=1):\n",
    "    print(f\"\\n=== 混合检索结果 {idx} ===\")\n",
    "    print(f\"内容: {result.page_content}\")\n",
    "    print(f\"混合分数: {result.metadata['hybrid_score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ae2381",
   "metadata": {},
   "source": [
    "计算命中率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d138da2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c3b7b5905048738cf78c2f6bda20e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   top_k  hit\n",
      "0      1  0.0\n",
      "1      2  0.0\n",
      "2      3  0.0\n",
      "3      4  0.0\n",
      "4      5  0.0\n",
      "5      6  0.0\n",
      "6      7  0.0\n",
      "7      8  0.0\n"
     ]
    }
   ],
   "source": [
    "# 初始化测试数据集\n",
    "import pandas as pd\n",
    "\n",
    "test_data = [\n",
    "    {\"question\": \"问题1\", \"uuid\": \"uuid1\"},\n",
    "    {\"question\": \"问题2\", \"uuid\": \"uuid2\"},\n",
    "    {\"question\": \"问题3\", \"uuid\": \"uuid3\"}\n",
    "]\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "# 在混合检索器基础上计算命中率\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 初始化统计数据\n",
    "top_k_arr = list(range(1, 9))\n",
    "hit_stat_data = []\n",
    "\n",
    "# 遍历测试集并计算命中率\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    question = row['question']\n",
    "    true_uuid = row['uuid']\n",
    "    chunks = hybrid_retriever.get_relevant_documents(question)\n",
    "    retrieved_uuids = [doc.metadata.get('uuid', '') for doc in chunks]\n",
    "\n",
    "    for k in top_k_arr:\n",
    "        hit_stat_data.append({\n",
    "            'question': question,\n",
    "            'top_k': k,\n",
    "            'hit': int(true_uuid in retrieved_uuids[:k])\n",
    "        })\n",
    "\n",
    "# 转换命中率数据为DataFrame\n",
    "hit_stat_df = pd.DataFrame(hit_stat_data)\n",
    "\n",
    "# 计算平均命中率\n",
    "average_hit_rate = hit_stat_df.groupby('top_k')['hit'].mean().reset_index()\n",
    "print(average_hit_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ed2dfc",
   "metadata": {},
   "source": [
    "以下是拓展检索器类以调整权重的板块，由于用户选择结果未定，所以暂时不跑通\n",
    "# 扩展检索器类以支持权重调整\n",
    "class CustomRetriever:\n",
    "    def __init__(self, retriever):\n",
    "        self.retriever = retriever\n",
    "        self.keyword_weight = 1.0\n",
    "        self.vector_weight = 1.0\n",
    "\n",
    "    def get_relevant_documents(self, query):\n",
    "        # 调用原始检索器的检索方法\n",
    "        return self.retriever.get_relevant_documents(query)\n",
    "\n",
    "    def adjust_weights(self, feedback):\n",
    "        # 根据反馈调整权重\n",
    "        results = self.get_relevant_documents(feedback[\"query\"])\n",
    "        if any(result.metadata.get('uuid') == feedback[\"preferred_result\"] for result in results):\n",
    "            self.keyword_weight += 0.1\n",
    "            self.vector_weight -= 0.1\n",
    "        print(f\"权重已调整: keyword_weight={self.keyword_weight}, vector_weight={self.vector_weight}\")\n",
    "\n",
    "# 包装原始检索器\n",
    "custom_retriever = CustomRetriever(retriever)\n",
    "\n",
    "# 模拟用户反馈\n",
    "user_feedback = {\n",
    "    \"preferred_result\": \"文档A\",  # 用户选择的结果\n",
    "    \"query\": \"2023年10月美国ISM制造业PMI指数较上月有何变化？\"\n",
    "}\n",
    "\n",
    "# 调整检索器权重\n",
    "custom_retriever.adjust_weights(user_feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c1ecc7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "以下是多轮检索的部分，由于出现内存不足的问题，所以暂时不跑通\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "# 修复LLM初始化问题\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "# 确保服务运行并检查 base_url\n",
    "try:\n",
    "    llm1 = Ollama(\n",
    "        model='qwen:7b',\n",
    "        base_url=\"http://localhost:11434\"\n",
    "    )\n",
    "    print(\"LLM1 加载成功\")\n",
    "except Exception as e:\n",
    "    print(f\"LLM1 加载失败: {e}\")\n",
    "\n",
    "# 定义多轮检索链\n",
    "retrieval_chain = ConversationalRetrievalChain.from_llm(\n",
    "    retriever=vector_db.as_retriever(search_kwargs={'k': 5}),\n",
    "    llm=llm1,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# 初始化对话历史\n",
    "chat_history = []\n",
    "\n",
    "# 模拟多轮对话\n",
    "queries = [\n",
    "    \"2023年10月美国ISM制造业PMI指数是多少？\",\n",
    "    \"与上月相比有什么变化？\",\n",
    "    \"这种变化的原因是什么？\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== 多轮对话 ===\")\n",
    "for query in queries:\n",
    "    try:\n",
    "        response = retrieval_chain({\"question\": query, \"chat_history\": chat_history})\n",
    "        print(f\"\\n用户问题: {query}\")\n",
    "        print(f\"模型回答: {response['answer']}\")\n",
    "        chat_history.append((query, response['answer']))\n",
    "    except Exception as e:\n",
    "        print(f\"对话失败: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6011eb1e",
   "metadata": {},
   "source": [
    "问答"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5950ca",
   "metadata": {},
   "source": [
    "流式输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3aeb21b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13972\\3274904193.py:10: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "是的，购买医疗保险后，通常会覆盖因重病引起的医疗费用，但具体保障范围和报销比例需要参照保险合同条款。"
     ]
    }
   ],
   "source": [
    "# 定义流式问答处理流程\n",
    "from langchain.llms import Ollama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "llm = Ollama(\n",
    "    model='qwen:7b',\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "prompt_tmpl = \"\"\"\n",
    "你是一名金融保险领域的专家，擅长根据所获取的信息片段，对问题进行分析和推理。\n",
    "你的任务是根据所获取的信息片段（<<<<context>>><<<</context>>>之间的内容）回答问题。\n",
    "回答保持简洁，不必重复问题，不要添加与答案无关的任何内容。\n",
    "已知信息：\n",
    "<<<<context>>>\n",
    "{context}\n",
    "<<<</context>>>\n",
    "\n",
    "问题：{question}\n",
    "请回答：\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_tmpl)\n",
    "retriever = vector_db.as_retriever(search_kwargs={'k': 4})\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in rag_chain.stream(\"购买医疗保险后，重病是否可以报销医疗费用？\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb9042fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "是的，购买医疗保险后，通常会覆盖因重大疾病发生的医疗费用，但具体情况可能根据保险条款有所不同。\n"
     ]
    }
   ],
   "source": [
    "# 非流式问答输出\n",
    "print(rag_chain.invoke('购买医疗保险后，重病是否可以报销医疗费用？'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
