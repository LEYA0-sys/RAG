{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8634d488",
   "metadata": {},
   "source": [
    "<font color=pink size=100>环境准备+代理</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8312e984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "langchain                     0.3.25\n",
      "langchain_community           0.3.25\n",
      "pypdf                         5.6.0\n",
      "sentence_transformers         4.1.0\n",
      "device: cpu\n",
      "device: cpu\n",
      "index.faiss 文件存在\n",
      "向量数据库中数据总数: 1163\n",
      "index.faiss 文件存在\n",
      "向量数据库中数据总数: 1163\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库并打印版本信息\n",
    "import langchain\n",
    "import langchain_community\n",
    "import pypdf\n",
    "import sentence_transformers\n",
    "print(1)\n",
    "for module in (langchain, langchain_community, pypdf, sentence_transformers):\n",
    "    print(f\"{module.__name__:<30}{module.__version__}\")\n",
    "\n",
    "#导入操作系统和数据处理相关库\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 定义嵌入模型\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import torch\n",
    "from langchain_ollama import OllamaLLM\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'device: {device}')\n",
    "#定义嵌入模型，跑通这部分代码需要开代理\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name='BAAI/bge-large-zh-v1.5',\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "#加载向量数据库\n",
    "# 加载 index.faiss 文件作为向量数据库\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "import faiss\n",
    "import os\n",
    "import numpy as np\n",
    "# 确保路径正确数据库文件\\index.faiss\n",
    "index_path = r\"E:\\RAG\\database\"\n",
    "if os.path.exists(index_path):\n",
    "    print(\"index.faiss 文件存在\")\n",
    "    vector_db = FAISS.load_local(index_path, embedding_model, allow_dangerous_deserialization=True)\n",
    "    \n",
    "# 输出向量数据库vector_db中包含的数据条数\n",
    "print(f\"向量数据库中数据总数: {vector_db.index.ntotal}\")\n",
    "\n",
    "# #采用ollama的LLM模型千问\n",
    "# llm = OllamaLLM(model=\"qwen:7b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda6e3b7",
   "metadata": {},
   "source": [
    "<font color=pink size=50>简单检索</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b79e8a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索结果已保存到 ../outputs/outputs_4.0/简单检索\\rag_results_easy_k=50.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import jieba\n",
    "from rank_bm25 import BM25Okapi\n",
    "import os\n",
    "import re\n",
    "\n",
    "#加载QA1，并提取页码信息\n",
    "qa_file_path = '../QA对/QA4.json'\n",
    "with open(qa_file_path, 'r', encoding='utf-8') as f:\n",
    "    qa_data = json.load(f)\n",
    "\n",
    "answers = [item['answer'] for item in qa_data]\n",
    "pages = [item['page_num'] for item in qa_data]  \n",
    "queries = [item['question'] for item in qa_data]\n",
    "k = 50 #返回检索结果的数量\n",
    "\n",
    "def jieba_tokenizer(text):\n",
    "    return list(jieba.cut(text))\n",
    "\n",
    "# 定义简单的RAG检索函数\n",
    "def rag_search(query, k=k):\n",
    "    \"\"\"\n",
    "    使用vector_db对输入query进行检索，返回前k条结果。\n",
    "    \"\"\"\n",
    "    retriever = vector_db.as_retriever(search_kwargs={\"k\": k})\n",
    "    results = retriever.invoke(query)\n",
    "    return results\n",
    "\n",
    "output_dir = '../outputs/outputs_4.0/简单检索'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file_path = os.path.join(output_dir, f'rag_results_easy_k={k}.json')\n",
    "\n",
    "recall_list = []\n",
    "output_data = []\n",
    "mrr_list = []\n",
    "\n",
    "# 新增：对每个问题进行检索，输出内容和页码\n",
    "for idx, (query, answer, correct_page) in enumerate(zip(queries, answers, pages)):\n",
    "    results = rag_search(query, k=k)\n",
    "    retrieved_documents = [\n",
    "        {\n",
    "            \"uuid\":doc.metadata.get(\"uuid\", \"未指定\"),\n",
    "            \"source_file\": doc.metadata.get(\"source_file\", \"\"),\n",
    "            \"source_content\": doc.page_content,\n",
    "            \"page_num\": str(doc.metadata.get(\"page_num\", \"未指定\"))\n",
    "        }\n",
    "        for doc in results\n",
    "    ]\n",
    "    retrieved_pages = [doc[\"page_num\"] for doc in retrieved_documents]\n",
    "    # 检查是否有检索到的页码与标准答案页码一致，若有recall=1，否则recall=0\n",
    "    recall = 1 if correct_page in retrieved_pages else 0\n",
    "    recall_list.append(recall)\n",
    "    # 计算MRR，即第一个正确答案的倒数排名\n",
    "    rank = 0\n",
    "    for i, p in enumerate(retrieved_pages):\n",
    "        if p == correct_page:\n",
    "            rank = i + 1\n",
    "            break\n",
    "    mrr = 1.0 / rank if rank > 0 else 0.0\n",
    "    mrr_list.append(mrr)\n",
    "    output_data.append({\n",
    "        \"query\": query,\n",
    "        \"answer\": answer,\n",
    "        \"correct_page\": correct_page,\n",
    "        \"recall\": recall,\n",
    "        \"mrr\": mrr,\n",
    "        \"retrieved_documents\": retrieved_documents\n",
    "    })\n",
    "\n",
    "avg_recall = sum(recall_list) / len(recall_list) if recall_list else 0.0\n",
    "avg_mrr = sum(mrr_list) / len(mrr_list) if mrr_list else 0.0\n",
    "\n",
    "# 保存整体指标到输出结果\n",
    "summary = {\n",
    "    \"avg_recall\": avg_recall,\n",
    "    \"avg_mrr\": avg_mrr\n",
    "}\n",
    "output = {\n",
    "    \"summary\": summary,\n",
    "    \"results\": output_data\n",
    "}\n",
    "with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(output, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"检索结果已保存到 {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111e4e70",
   "metadata": {},
   "source": [
    "<font color=pink size=20>混合检索</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b817294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "import jieba\n",
    "import json\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from multiprocessing import cpu_count\n",
    "from functools import partial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "k=50  # 返回检索结果的数量\n",
    "def tokenize(text):\n",
    "    \"\"\"使用jieba进行中文分词\"\"\"\n",
    "    return list(jieba.cut(text))\n",
    "\n",
    "def create_bm25_index(documents):\n",
    "    \"\"\"创建BM25索引\"\"\"\n",
    "    tokenized_docs = [tokenize(doc.page_content) for doc in documents]\n",
    "    return BM25Okapi(tokenized_docs)\n",
    "\n",
    "def hybrid_search(query, bm25_index, documents, embeddings, embedding_model, k=20, alpha=0.75):\n",
    "    \"\"\"混合检索实现\"\"\"\n",
    "    # BM25检索\n",
    "    tokenized_query = tokenize(query)\n",
    "    bm25_scores = bm25_index.get_scores(tokenized_query)\n",
    "    \n",
    "    # Embedding检索\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    # 确保embeddings是numpy数组\n",
    "    embeddings = np.array(embeddings)\n",
    "    query_embedding = np.array(query_embedding).reshape(1, -1)\n",
    "    \n",
    "    # 计算余弦相似度\n",
    "    embedding_scores = cosine_similarity(query_embedding, embeddings)[0]\n",
    "    \n",
    "    # 归一化分数\n",
    "    bm25_scores = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min() + 1e-6)\n",
    "    embedding_scores = (embedding_scores - embedding_scores.min()) / (embedding_scores.max() - embedding_scores.min() + 1e-6)\n",
    "    \n",
    "    # 混合分数\n",
    "    hybrid_scores = alpha * bm25_scores + (1 - alpha) * embedding_scores\n",
    "    \n",
    "    # 获取top-k结果\n",
    "    top_k_indices = np.argsort(hybrid_scores)[-k:][::-1]\n",
    "    \n",
    "    # 构建结果\n",
    "    results = []\n",
    "    for idx in top_k_indices:\n",
    "        doc = documents[idx]\n",
    "        results.append({\n",
    "            \"uuid\": doc.metadata.get(\"uuid\", \"未指定\"),\n",
    "            \"source_file\": doc.metadata.get(\"source_file\", \"\"),\n",
    "            \"source_content\": doc.page_content,\n",
    "            \"page_num\": str(doc.metadata.get(\"page_num\", \"未指定\")),\n",
    "            \"score\": float(hybrid_scores[idx]),\n",
    "            \"bm25_score\": float(bm25_scores[idx]),\n",
    "            \"embedding_score\": float(embedding_scores[idx])\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def process_hybrid_search(query, bm25_index, documents, embeddings, embedding_model, k=20):\n",
    "    \"\"\"处理单个查询的混合检索\"\"\"\n",
    "    try:\n",
    "        results = hybrid_search(\n",
    "            query=query,\n",
    "            bm25_index=bm25_index,\n",
    "            documents=documents,\n",
    "            embeddings=embeddings,\n",
    "            embedding_model=embedding_model,\n",
    "            k=k\n",
    "        )\n",
    "        return {\n",
    "            'query': query,\n",
    "            'retrieved_documents': results\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"处理查询时出错: {str(e)}\")\n",
    "        return {\n",
    "            'query': query,\n",
    "            'retrieved_documents': [],\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def parallel_hybrid_search(queries, documents, embeddings, embedding_model, k=k):\n",
    "    \"\"\"并行处理多个查询的混合检索\"\"\"\n",
    "    # 创建BM25索引\n",
    "    bm25_index = create_bm25_index(documents)\n",
    "    \n",
    "    # 确保embeddings是numpy数组\n",
    "    embeddings = np.array(embeddings)\n",
    "    \n",
    "    # 使用线程池并行处理所有查询\n",
    "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n",
    "        # 使用partial固定参数\n",
    "        process_func = partial(\n",
    "            process_hybrid_search,\n",
    "            bm25_index=bm25_index,\n",
    "            documents=documents,\n",
    "            embeddings=embeddings,\n",
    "            embedding_model=embedding_model,\n",
    "            k=k\n",
    "        )\n",
    "        results = list(executor.map(process_func, queries))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    # 加载QA4.json中的问题\n",
    "    qa_file_path = '../QA对/QA4.json'\n",
    "    with open(qa_file_path, 'r', encoding='utf-8') as f:\n",
    "        qa_data = json.load(f)\n",
    "    \n",
    "    # 提取问题和答案\n",
    "    queries = [item['question'] for item in qa_data]\n",
    "    answers = [item['answer'] for item in qa_data]\n",
    "    pages = [item['page_num'] for item in qa_data]\n",
    "    \n",
    "    # 获取所有文档\n",
    "    all_docs = list(vector_db.docstore._dict.values())\n",
    "    \n",
    "    # 获取所有文档的embeddings\n",
    "    all_texts = [doc.page_content for doc in all_docs]\n",
    "    embeddings = embedding_model.embed_documents(all_texts)\n",
    "    \n",
    "    # 执行混合检索\n",
    "    results = parallel_hybrid_search(\n",
    "        queries=queries,\n",
    "        documents=all_docs,\n",
    "        embeddings=embeddings,\n",
    "        embedding_model=embedding_model,\n",
    "        k=k\n",
    "    )\n",
    "    \n",
    "    # 计算评估指标\n",
    "    recall_list = []\n",
    "    mrr_list = []\n",
    "    output_data = []\n",
    "    \n",
    "    for idx, (result, answer, correct_page) in enumerate(zip(results, answers, pages)):\n",
    "        retrieved_pages = [doc[\"page_num\"] for doc in result['retrieved_documents']]\n",
    "        \n",
    "        # 计算recall\n",
    "        recall = 1 if correct_page in retrieved_pages else 0\n",
    "        recall_list.append(recall)\n",
    "        \n",
    "        # 计算MRR\n",
    "        rank = 0\n",
    "        for i, p in enumerate(retrieved_pages):\n",
    "            if p == correct_page:\n",
    "                rank = i + 1\n",
    "                break\n",
    "        mrr = 1.0 / rank if rank > 0 else 0.0\n",
    "        mrr_list.append(mrr)\n",
    "        \n",
    "        # 构建输出数据\n",
    "        output_data.append({\n",
    "            \"query\": result['query'],\n",
    "            \"answer\": answer,\n",
    "            \"correct_page\": correct_page,\n",
    "            \"recall\": recall,\n",
    "            \"mrr\": mrr,\n",
    "            \"retrieved_documents\": result['retrieved_documents']\n",
    "        })\n",
    "    \n",
    "    # 计算平均指标\n",
    "    avg_recall = sum(recall_list) / len(recall_list) if recall_list else 0.0\n",
    "    avg_mrr = sum(mrr_list) / len(mrr_list) if mrr_list else 0.0\n",
    "    \n",
    "    # 保存结果\n",
    "    output_dir = '../outputs/outputs_4.0/混合检索'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file_path = os.path.join(output_dir, f'hybrid_results_alpha=0.75_k={k}.json')\n",
    "    \n",
    "    output = {\n",
    "        \"summary\": {\n",
    "            \"avg_recall\": avg_recall,\n",
    "            \"avg_mrr\": avg_mrr\n",
    "        },\n",
    "        \"results\": output_data\n",
    "    }\n",
    "    \n",
    "    with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"混合检索结果已保存到 {output_file_path}\")\n",
    "    print(f\"平均Recall: {avg_recall:.4f}\")\n",
    "    print(f\"平均MRR: {avg_mrr:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc9885b",
   "metadata": {},
   "source": [
    "<font color=pink size=20 >多查询检索+混合检索</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69e69c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用多套提示词，添加MRR和Recall指标，生成输出文件.json\n",
    "from typing import List\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "import concurrent.futures\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from typing import Tuple, Dict, List, Any\n",
    "from typing import List, Dict\n",
    "from langchain.prompts import PromptTemplate\n",
    "k = 50\n",
    "\n",
    "# 定义提示词模板\n",
    "PROMPT_TEMPLATE = \"\"\"您是查询扩展方面的专家，能够生成问题的释义。\n",
    "我无法直接使用用户的问题从知识库中检索相关信息。\n",
    "您需要通过多种方式扩展或释义用户的问题，例如使用同义词/短语、完整地写出缩写、添加一些额外的描述或解释、改变表达方式、将原始问题翻译成中文等。\n",
    "并返回 5 个版本的问题，其中一个来自翻译。\n",
    "只需列出问题。不需要其他单词。\n",
    "\n",
    "原始问题：{query}\"\"\"\n",
    "\n",
    "\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return list(filter(None, lines))\n",
    "\n",
    "output_parser = LineListOutputParser()\n",
    "\n",
    "API_KEY = \"3757050b-82ca-41c0-a872-065b7913ff87\"\n",
    "MODEL_NAME = \"doubao-1.5-thinking-pro-m-250428\"\n",
    "\n",
    "import os\n",
    "from volcenginesdkarkruntime import Ark\n",
    "\n",
    "# 创建方舟客户端\n",
    "client = Ark(\n",
    "    api_key=API_KEY,  # 使用已定义的 API_KEY\n",
    "    timeout=1800,     # 设置30分钟超时\n",
    ")\n",
    "\n",
    "def call_llm_api(prompt: str, max_retries: int = 3) -> str:\n",
    "    \"\"\"调用LLM API生成回答\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL_NAME,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"你是一个专业的保险问题生成助手。\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise Exception(f\"API调用失败: {str(e)}\")\n",
    "            time.sleep(2 ** attempt)  # 指数退避\n",
    "\n",
    "\n",
    "def calculate_metrics(documents, correct_page):\n",
    "    \"\"\"计算召回率和MRR\"\"\"\n",
    "    retrieved_pages = [str(doc.metadata.get(\"page_num\", \"\")) for doc in documents]\n",
    "    recall = 1 if correct_page in retrieved_pages else 0\n",
    "    \n",
    "    rank = 0\n",
    "    for i, p in enumerate(retrieved_pages):\n",
    "        if p == correct_page:\n",
    "            rank = i + 1\n",
    "            break\n",
    "    mrr = 1.0 / rank if rank > 0 else 0.0\n",
    "    \n",
    "    return recall, mrr\n",
    "\n",
    "\n",
    "def retrieve_documents(question: str, retriever) -> List[Any]:\n",
    "    \"\"\"单个问题的检索函数\"\"\"\n",
    "    try:\n",
    "        # 使用混合检索，同时考虑关键词匹配和语义相似度\n",
    "        docs = retriever.get_relevant_documents(\n",
    "            question,\n",
    "            search_type=\"hybrid\",  # 使用混合检索\n",
    "            search_kwargs={\n",
    "                \"k\": k,\n",
    "                \"fetch_k\": k * 2,  # 获取更多候选文档\n",
    "                \"lambda_mult\": 0.5,  # 平衡关键词和语义相似度的权重\n",
    "                \"hybrid_search_type\": \"both\"  # 同时使用关键词和语义搜索\n",
    "            }\n",
    "        )\n",
    "        return docs\n",
    "    except Exception as e:\n",
    "        print(f\"检索出错: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def process_query(query: str, correct_page: str, answer: str, retriever) -> Tuple[Dict, float, float]:\n",
    "    \"\"\"处理单个查询\"\"\"\n",
    "    try:\n",
    "        # 使用提示词模板生成问题\n",
    "        prompt = PROMPT_TEMPLATE.format(query=query)\n",
    "        generated_questions = call_llm_api(prompt).strip().split('\\n')\n",
    "        \n",
    "        # 并行检索所有生成的问题\n",
    "        raw_documents = []\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=min(5, len(generated_questions))) as executor:\n",
    "            # 创建检索任务\n",
    "            future_to_question = {\n",
    "                executor.submit(retrieve_documents, question, retriever): question \n",
    "                for question in generated_questions\n",
    "            }\n",
    "            \n",
    "            # 收集检索结果\n",
    "            for future in concurrent.futures.as_completed(future_to_question):\n",
    "                docs = future.result()\n",
    "                raw_documents.extend(docs)\n",
    "        \n",
    "        # 文档去重\n",
    "        seen_contents = set()\n",
    "        unique_documents = []\n",
    "        for doc in raw_documents:\n",
    "            content = doc.page_content.strip()\n",
    "            if content not in seen_contents:\n",
    "                seen_contents.add(content)\n",
    "                unique_documents.append(doc)\n",
    "        \n",
    "        # 计算召回率和MRR\n",
    "        recall, mrr = calculate_metrics(unique_documents, correct_page)\n",
    "         # 将 Document 对象转换为可序列化的字典\n",
    "        serializable_docs = []\n",
    "        for doc in unique_documents:\n",
    "            serializable_docs.append({\n",
    "                \"page_content\": doc.page_content,\n",
    "                \"metadata\": doc.metadata\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"correct_page\": correct_page,\n",
    "            \"answer\": answer,  # 添加标准答案\n",
    "            \"recall\": recall,\n",
    "            \"mrr\": mrr,\n",
    "            \"generated_questions\": generated_questions,\n",
    "            \"retrieved_documents\": serializable_docs  # 使用可序列化的文档列表\n",
    "        }, recall, mrr\n",
    "    except Exception as e:\n",
    "        print(f\"处理查询时出错: {str(e)}\")\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"correct_page\": correct_page,\n",
    "            \"answer\": answer,  # 添加标准答案\n",
    "            \"recall\": 0,\n",
    "            \"mrr\": 0,\n",
    "            \"generated_questions\": [],\n",
    "            \"retrieved_documents\": [],\n",
    "            \"error\": str(e)\n",
    "        }, 0, 0\n",
    "\n",
    "def evaluate_queries(queries, correct_pages, answers, retriever):\n",
    "    \"\"\"评估查询结果\"\"\"\n",
    "    results = []\n",
    "    total_recall = 0\n",
    "    total_mrr = 0\n",
    "    \n",
    "    for query, correct_page, answer in zip(queries, correct_pages, answers):\n",
    "        result, recall, mrr = process_query(query, correct_page, answer, retriever)\n",
    "        results.append(result)\n",
    "        total_recall += recall\n",
    "        total_mrr += mrr\n",
    "    \n",
    "    avg_recall = total_recall / len(queries)\n",
    "    avg_mrr = total_mrr / len(queries)\n",
    "    \n",
    "    return results, avg_recall, avg_mrr\n",
    "\n",
    "# 执行评估\n",
    "# 创建检索器\n",
    "# 加载QA数据\n",
    "qa_data_path =  '../QA对/QA4.json'\n",
    "with open(qa_data_path, 'r', encoding='utf-8') as f:\n",
    "    qa_data = json.load(f)\n",
    "\n",
    "# 提取查询、答案和正确页码\n",
    "queries = [item['question'] for item in qa_data]\n",
    "answers = [item['answer'] for item in qa_data]\n",
    "pages = [item['page_num'] for item in qa_data]\n",
    "\n",
    "# 执行评估\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": k})\n",
    "results, avg_recall, avg_mrr = evaluate_queries(queries, pages, answers, retriever)\n",
    "\n",
    "# 保存结果\n",
    "output = {\n",
    "    \"summary\": {\n",
    "        \"avg_recall\": avg_recall,\n",
    "        \"avg_mrr\": avg_mrr,\n",
    "        \"total_questions\": len(qa_data)\n",
    "    },\n",
    "    \"results\": results\n",
    "}\n",
    "\n",
    "# 确保输出目录存在\n",
    "output_dir = '../outputs/outputs_4.0/多提示词检索'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 保存结果到文件\n",
    "output_file_path = os.path.join(output_dir, f'multi_prompt_results_k={k}.json')\n",
    "with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(output, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"评估完成！结果已保存到: {output_file_path}\")\n",
    "print(f\"平均Recall: {avg_recall:.4f}\")\n",
    "print(f\"平均MRR: {avg_mrr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0c68aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install volcenginesdk-ark-runtime\n",
    "\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "import jieba\n",
    "import json\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from multiprocessing import cpu_count\n",
    "from functools import partial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Dict\n",
    "from volcenginesdkarkruntime import Ark\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "\n",
    "# 配置\n",
    "API_KEY = \"3757050b-82ca-41c0-a872-065b7913ff87\"\n",
    "MODEL_NAME = \"deepseek-r1-distill-qwen-32b-250120\"\n",
    "k = 50  # 返回检索结果的数量\n",
    "\n",
    "# 创建方舟客户端\n",
    "client = Ark(\n",
    "    api_key=API_KEY,\n",
    "    timeout=1800,\n",
    ")\n",
    "\n",
    "# 定义提示词模板\n",
    "PROMPT_TEMPLATE = \"\"\"您是查询扩展方面的专家，能够生成问题的释义。\n",
    "我无法直接使用用户的问题从知识库中检索相关信息。\n",
    "您需要通过多种方式扩展或释义用户的问题，例如使用同义词/短语、完整地写出缩写、添加一些额外的描述或解释、改变表达方式、将原始问题翻译成中文等。\n",
    "并返回 5 个版本的问题，其中一个来自翻译。\n",
    "只需列出问题。不需要其他单词。\n",
    "\n",
    "原始问题：{query}\"\"\"\n",
    "\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return list(filter(None, lines))\n",
    "\n",
    "output_parser = LineListOutputParser()\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"使用jieba进行中文分词\"\"\"\n",
    "    return list(jieba.cut(text))\n",
    "\n",
    "def create_bm25_index(documents):\n",
    "    \"\"\"创建BM25索引\"\"\"\n",
    "    tokenized_docs = [tokenize(doc.page_content) for doc in documents]\n",
    "    return BM25Okapi(tokenized_docs)\n",
    "\n",
    "def llm_rerank_api(query: str, results: List[Dict], top_k: int = k) -> List[Dict]:\n",
    "    \"\"\"使用API调用LLM对检索结果进行重排序\"\"\"\n",
    "    try:\n",
    "        # 构建重排序提示词\n",
    "        rerank_prompt = f\"\"\"你是一个专业的文档相关性评估专家。请评估以下文档与问题的相关性，并给出0-1之间的分数。\n",
    "问题：{query}\n",
    "\n",
    "请对每个文档进行评分，分数越高表示与问题越相关。评分标准：\n",
    "1. 文档内容是否直接回答了问题\n",
    "2. 文档内容与问题的语义相关性\n",
    "3. 文档内容的完整性和准确性\n",
    "\n",
    "文档列表：\n",
    "\"\"\"\n",
    "        \n",
    "        # 为每个文档添加编号和内容\n",
    "        for i, doc in enumerate(results):\n",
    "            rerank_prompt += f\"\\n文档{i+1}：\\n{doc['source_content']}\\n\"\n",
    "        \n",
    "        rerank_prompt += \"\\n请给出每个文档的评分（0-1之间的小数），格式为：文档编号:分数\"\n",
    "        \n",
    "        # 调用API进行重排序\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"你是一个专业的文档相关性评估专家。\"},\n",
    "                {\"role\": \"user\", \"content\": rerank_prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # 解析API返回的评分\n",
    "        scores_text = response.choices[0].message.content\n",
    "        scores = {}\n",
    "        for line in scores_text.split('\\n'):\n",
    "            if ':' in line:\n",
    "                doc_num, score = line.split(':')\n",
    "                try:\n",
    "                    doc_num = int(doc_num.strip().replace('文档', ''))\n",
    "                    score = float(score.strip())\n",
    "                    scores[doc_num] = score\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        # 将LLM评分添加到结果中\n",
    "        for i, doc in enumerate(results):\n",
    "            doc['llm_score'] = scores.get(i+1, 0.0)\n",
    "        \n",
    "        # 根据LLM评分重排序\n",
    "        reranked_results = sorted(results, key=lambda x: x['llm_score'], reverse=True)\n",
    "        \n",
    "        return reranked_results[:top_k]\n",
    "    except Exception as e:\n",
    "        print(f\"LLM重排序出错: {str(e)}\")\n",
    "        return results[:top_k]  # 如果重排序失败，返回原始排序结果\n",
    "\n",
    "def hybrid_search(query, bm25_index, documents, embeddings, embedding_model, k=k, alpha=0.5):\n",
    "    \"\"\"混合检索实现\"\"\"\n",
    "    # BM25检索\n",
    "    tokenized_query = tokenize(query)\n",
    "    bm25_scores = bm25_index.get_scores(tokenized_query)\n",
    "    \n",
    "    # Embedding检索\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    # 确保embeddings是numpy数组\n",
    "    embeddings = np.array(embeddings)\n",
    "    query_embedding = np.array(query_embedding).reshape(1, -1)\n",
    "    \n",
    "    # 计算余弦相似度\n",
    "    embedding_scores = cosine_similarity(query_embedding, embeddings)[0]\n",
    "    \n",
    "    # 归一化分数\n",
    "    bm25_scores = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min() + 1e-6)\n",
    "    embedding_scores = (embedding_scores - embedding_scores.min()) / (embedding_scores.max() - embedding_scores.min() + 1e-6)\n",
    "    \n",
    "    # 混合分数\n",
    "    hybrid_scores = alpha * bm25_scores + (1 - alpha) * embedding_scores\n",
    "    \n",
    "    # 获取top-k结果\n",
    "    top_k_indices = np.argsort(hybrid_scores)[-k:][::-1]\n",
    "    \n",
    "    # 构建结果\n",
    "    results = []\n",
    "    for idx in top_k_indices:\n",
    "        doc = documents[idx]\n",
    "        results.append({\n",
    "            \"uuid\": doc.metadata.get(\"uuid\", \"未指定\"),\n",
    "            \"source_file\": doc.metadata.get(\"source_file\", \"\"),\n",
    "            \"source_content\": doc.page_content,\n",
    "            \"page_num\": str(doc.metadata.get(\"page_num\", \"未指定\")),\n",
    "            \"score\": float(hybrid_scores[idx]),\n",
    "            \"bm25_score\": float(bm25_scores[idx]),\n",
    "            \"embedding_score\": float(embedding_scores[idx])\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def multi_prompt_retrieve(query, bm25_index, documents, embeddings, embedding_model, k=k):\n",
    "    \"\"\"多查询检索实现\"\"\"\n",
    "    # 使用固定的prompt模板进行查询扩展\n",
    "    prompt = PromptTemplate(input_variables=[\"query\"], template=PROMPT_TEMPLATE)\n",
    "    llm_chain = prompt | llm | output_parser\n",
    "    expanded_queries = llm_chain.invoke(query)\n",
    "    \n",
    "    # 对每个扩展问题分别进行混合检索，合并去重\n",
    "    doc_set = {}\n",
    "    for q in expanded_queries:\n",
    "        # 使用hybrid_search替代hybrid_retrieve\n",
    "        results = hybrid_search(q, bm25_index, documents, embeddings, embedding_model, k=k)\n",
    "        for doc in results:\n",
    "            doc_id = doc[\"uuid\"] + doc[\"page_num\"]\n",
    "            if doc_id not in doc_set:\n",
    "                doc_set[doc_id] = doc\n",
    "    \n",
    "    # 将结果转换为列表\n",
    "    results = list(doc_set.values())\n",
    "    \n",
    "    # 使用LLM进行重排序\n",
    "    reranked_results = llm_rerank_api(query, results, top_k=k)\n",
    "    \n",
    "    return reranked_results\n",
    "\n",
    "def main():\n",
    "    # 加载QA4.json中的问题\n",
    "    qa_file_path = '../QA对/QA4.json'\n",
    "    with open(qa_file_path, 'r', encoding='utf-8') as f:\n",
    "        qa_data = json.load(f)\n",
    "    \n",
    "    # 提取问题和答案\n",
    "    queries = [item['question'] for item in qa_data]\n",
    "    answers = [item['answer'] for item in qa_data]\n",
    "    pages = [item['page_num'] for item in qa_data]\n",
    "    \n",
    "    # 获取所有文档\n",
    "    all_docs = list(vector_db.docstore._dict.values())\n",
    "    \n",
    "    # 创建BM25索引\n",
    "    bm25_index = create_bm25_index(all_docs)\n",
    "    \n",
    "    # 获取所有文档的embeddings\n",
    "    all_texts = [doc.page_content for doc in all_docs]\n",
    "    embeddings = embedding_model.embed_documents(all_texts)\n",
    "    \n",
    "    # 执行多查询混合检索\n",
    "    results = []\n",
    "    for query in queries:\n",
    "        result = multi_prompt_retrieve(\n",
    "            query, \n",
    "            bm25_index, \n",
    "            all_docs, \n",
    "            embeddings, \n",
    "            embedding_model, \n",
    "            k=k\n",
    "        )\n",
    "        results.append({\n",
    "            'query': query,\n",
    "            'retrieved_documents': result\n",
    "        })\n",
    "    \n",
    "    # 计算评估指标\n",
    "    recall_list = []\n",
    "    mrr_list = []\n",
    "    output_data = []\n",
    "    \n",
    "    for idx, (result, answer, correct_page) in enumerate(zip(results, answers, pages)):\n",
    "        retrieved_pages = [doc[\"page_num\"] for doc in result['retrieved_documents']]\n",
    "        \n",
    "        # 计算recall\n",
    "        recall = 1 if correct_page in retrieved_pages else 0\n",
    "        recall_list.append(recall)\n",
    "        \n",
    "        # 计算MRR\n",
    "        rank = 0\n",
    "        for i, p in enumerate(retrieved_pages):\n",
    "            if p == correct_page:\n",
    "                rank = i + 1\n",
    "                break\n",
    "        mrr = 1.0 / rank if rank > 0 else 0.0\n",
    "        mrr_list.append(mrr)\n",
    "        \n",
    "        # 构建输出数据\n",
    "        output_data.append({\n",
    "            \"query\": result['query'],\n",
    "            \"answer\": answer,\n",
    "            \"correct_page\": correct_page,\n",
    "            \"recall\": recall,\n",
    "            \"mrr\": mrr,\n",
    "            \"retrieved_documents\": result['retrieved_documents']\n",
    "        })\n",
    "    \n",
    "    # 计算平均指标\n",
    "    avg_recall = sum(recall_list) / len(recall_list) if recall_list else 0.0\n",
    "    avg_mrr = sum(mrr_list) / len(mrr_list) if mrr_list else 0.0\n",
    "    \n",
    "    # 保存结果\n",
    "    output_dir = '../outputs/outputs_4.0/多查询_混合检索_LLM重排序'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file_path = os.path.join(output_dir, f'multi_hybrid_llm_rerank_results_k={k}.json')\n",
    "    \n",
    "    output = {\n",
    "        \"summary\": {\n",
    "            \"avg_recall\": avg_recall,\n",
    "            \"avg_mrr\": avg_mrr\n",
    "        },\n",
    "        \"results\": output_data\n",
    "    }\n",
    "    \n",
    "    with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"多查询混合检索+LLM重排序结果已保存到 {output_file_path}\")\n",
    "    print(f\"平均Recall: {avg_recall:.4f}\")\n",
    "    print(f\"平均MRR: {avg_mrr:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2019253",
   "metadata": {},
   "source": [
    "<font color=pink size=20 >上下文压缩</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c73a37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用API方式压缩文档\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain_core.documents import Document \n",
    "import json\n",
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "import concurrent.futures\n",
    "import random\n",
    "import requests\n",
    "API_KEY = \"3757050b-82ca-41c0-a872-065b7913ff87\"\n",
    "API_BASE = \"https://ark.cn-beijing.volces.com/api/v3\"\n",
    "MODEL_NAME = \"deepseek-r1-250120\"\n",
    "def compress_with_api(doc, query, api_key, api_base, model_name):\n",
    "    \"\"\"\n",
    "    使用API调用大语言模型对单个文档进行压缩。\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"请根据以下问题和文档内容，提取与问题最相关的内容，去除无关信息，输出精炼后的内容。\\n\"\n",
    "        f\"问题：{query}\\n\"\n",
    "        f\"文档内容：{doc.page_content}\\n\"\n",
    "        f\"压缩后内容：\"\n",
    "    )\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"你是一个文档压缩助手，善于根据问题压缩文档内容。\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": 0,\n",
    "        \"max_tokens\": 1024\n",
    "    }\n",
    "    response = requests.post(\n",
    "        f\"{API_BASE}/chat/completions\",\n",
    "        headers=headers,\n",
    "        json=payload,\n",
    "        timeout=60\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        return result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    else:\n",
    "        print(f\"API调用失败: {response.status_code}, {response.text}\")\n",
    "        return doc.page_content  # 失败时返回原文\n",
    "\n",
    "def compress_document(doc, query):\n",
    "    \"\"\"API方式压缩单个文档\"\"\"\n",
    "    try:\n",
    "        compressed_content = compress_with_api(\n",
    "            doc, query,\n",
    "            api_key=API_KEY,\n",
    "            api_base=API_BASE,\n",
    "            model_name=MODEL_NAME\n",
    "        )\n",
    "        return Document(\n",
    "            page_content=compressed_content,\n",
    "            metadata=doc.metadata\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"API压缩文档时出错: {str(e)}\")\n",
    "        return doc\n",
    "\n",
    "def parallel_compress_documents(docs, query, num_threads=8):\n",
    "    \"\"\"并行压缩多个文档（API方式，线程池）\"\"\"\n",
    "    import concurrent.futures\n",
    "    if len(docs) > 10:\n",
    "        docs = random.sample(docs, 10)\n",
    "    results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = [executor.submit(compress_document, doc, query) for doc in docs]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            results.append(future.result())\n",
    "    return results\n",
    "\n",
    "def compress_one(item):\n",
    "    \"\"\"压缩单个查询的所有文档\"\"\"\n",
    "    query = item['query']\n",
    "    docs = []\n",
    "    for doc_dict in item['retrieved_documents']:\n",
    "        doc = Document(\n",
    "            page_content=doc_dict.get('content', ''),\n",
    "            metadata={\n",
    "                \"uuid\": doc_dict.get(\"uuid\", \"未指定\"),\n",
    "                \"source_file\": doc_dict.get(\"source_file\", \"\"),\n",
    "                \"page\": doc_dict.get(\"page\", \"未指定\")\n",
    "            }\n",
    "        )\n",
    "        docs.append(doc)\n",
    "    compressed_docs = parallel_compress_documents(docs, query)\n",
    "    for i, doc_dict in enumerate(item['retrieved_documents']):\n",
    "        if i < len(compressed_docs):\n",
    "            doc_dict['compressed_content'] = compressed_docs[i].page_content\n",
    "    return item\n",
    "\n",
    "def process_compression(input_file_path, output_file_path, k=20):\n",
    "    \"\"\"主处理函数（API方式）\"\"\"\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    import concurrent.futures\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        results = list(executor.map(compress_one, data['results']))\n",
    "    data['results'] = results\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"压缩结果已保存到 {output_file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 设置输入输出路径\n",
    "    input_file_path = f'../outputs/outputs_4.0/混合检索/hybrid_results_k={k}.json'\n",
    "    output_dir = '../outputs/outputs_4.0/上下文压缩'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file_path = os.path.join(output_dir, f'compressed_results_k={k}.json')\n",
    "    process_compression(\n",
    "        input_file_path=input_file_path,\n",
    "        output_file_path=output_file_path,\n",
    "        k=k\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785c1444",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "<font color=pink size=20 >添加问答链</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1d4f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from typing import List, Dict, Tuple\n",
    "from volcenginesdkarkruntime import Ark # 方舟API客户端\n",
    "from nltk.translate.bleu_score import sentence_bleu # BLEU评分\n",
    "from rouge_chinese import Rouge # 中文ROUGE评分\n",
    "import jieba # 中文分词\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# 配置\n",
    "API_KEY = \"3757050b-82ca-41c0-a872-065b7913ff87\"\n",
    "MODEL_NAME = \"doubao-1.5-thinking-vision-pro-250428\"\n",
    "\n",
    "# 创建方舟客户端\n",
    "client = Ark(\n",
    "    api_key=API_KEY,\n",
    "    timeout=1800,\n",
    ")\n",
    "\n",
    "# 加载QA数据\n",
    "qa_data_path = \"../QA对/QA4.json\"\n",
    "with open(qa_data_path, 'r', encoding='utf-8') as f:\n",
    "    qa_data = json.load(f)\n",
    "\n",
    "    \n",
    "# 加载混合检索结果\n",
    "hybrid_results_path = \"../outputs/outputs_4.0/混合检索/hybrid_results_k=50.json\"\n",
    "with open(hybrid_results_path, 'r', encoding='utf-8') as f:\n",
    "    hybrid_results = json.load(f)\n",
    "\n",
    "\n",
    "def calculate_similarity_metrics(predicted: str, reference: str) -> Dict[str, float]:\n",
    "    \"\"\"计算BLEU和ROUGE分数\"\"\"\n",
    "    # 先对标准答案和生成答案进行jieba分词\n",
    "    pred_tokens = list(jieba.cut(predicted))\n",
    "    ref_tokens = list(jieba.cut(reference))\n",
    "    \n",
    "    # 在计算BLEU分数时添加平滑函数\n",
    "    # 平滑函数（SmoothingFunction）用于在BLEU分数计算中避免因高阶n-gram（如2-gram、3-gram等）没有匹配而导致分数为0的情况。\n",
    "    # 它通过对分母加1等方式，使得即使没有高阶匹配也能得到较小但非零的分数，从而更合理地反映生成文本与参考答案的相似度。\n",
    "    # 高阶n-gram（如2-gram、3-gram等）是指由连续n个词组成的短语。例如，2-gram是连续两个词的组合，3-gram是连续三个词的组合。BLEU分数会统计这些n-gram在生成文本和参考答案中的重合情况，n越大，匹配难度越高，但能更好地反映语序和上下文的准确性。\n",
    "    bleu_score = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=SmoothingFunction().method1)\n",
    "    \n",
    "    # 计算ROUGE\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = rouge.get_scores(predicted, reference)[0]\n",
    "    \n",
    "    return {\n",
    "        \"bleu\": bleu_score,#关注精确匹配和词序，可能忽略同义词\n",
    "        \"rouge-1\": rouge_scores[\"rouge-1\"][\"f\"],#关注词汇覆盖（单个词的匹配程度）\n",
    "        \"rouge-2\": rouge_scores[\"rouge-2\"][\"f\"],#关注词语准确性（两个词的匹配程度）\n",
    "        \"rouge-l\": rouge_scores[\"rouge-l\"][\"f\"] #关注整体语义连贯性（最长公共子序列的匹配长度）\n",
    "    }\n",
    "\n",
    "# 加载检索结果\n",
    "hybrid_results_path = \"../outputs/outputs_4.0/混合检索/hybrid_results_k=50.json\"\n",
    "with open(hybrid_results_path, 'r', encoding='utf-8') as f:\n",
    "    hybrid_results = json.load(f)\n",
    "\n",
    "def evaluate_qa_chain(qa_data: List[Dict], hybrid_results: List[Dict]) -> Dict:\n",
    "    \"\"\"评估问答链性能\"\"\"\n",
    "    results = []\n",
    "    total_metrics = {\n",
    "        \"bleu\": 0.0,\n",
    "        \"rouge-1\": 0.0,\n",
    "        \"rouge-2\": 0.0,\n",
    "        \"rouge-l\": 0.0\n",
    "    }\n",
    "    \n",
    "    # 创建问题到检索结果的映射\n",
    "    results_map = {item[\"query\"]: item for item in hybrid_results[\"results\"]}\n",
    "    \n",
    "    for item in qa_data:\n",
    "        try:\n",
    "            # 获取该问题的检索结果\n",
    "            query_result = results_map.get(item[\"question\"])\n",
    "            if not query_result:\n",
    "                print(f\"未找到问题 '{item['question']}' 的检索结果\")\n",
    "                continue\n",
    "                \n",
    "            # 构建提示词\n",
    "            context = \"\\n\".join([doc[\"source_content\"] for doc in query_result[\"retrieved_documents\"]])\n",
    "            prompt = f\"\"\"你是一名金融保险领域的专家，擅长根据所获取的信息片段，对问题进行分析和推理。你的任务是根据所获取的上下文信息回答问题。\n",
    "回答保持简洁，不必重复问题，不要添加与问题无关的任何内容。\n",
    "如果上下文信息不足以回答问题，请明确说明。\n",
    "如果上下文信息中包含多个片段，请综合考虑所有片段的信息来回答问题。\n",
    "如果所提供的上下文中存在矛盾的信息，请指出并解释原因。\n",
    "如果上下文信息中存在多余的信息，如符号、标点、空格等，请先判断是否可以忽略，确保最终输出结果清晰易懂，是流畅的内容。\n",
    "如果上下文信息中存在不必要的细节或背景信息，请忽略这些信息。\n",
    "如果上下文信息中存在繁体中文，请将其转换为简体字。\n",
    "如果上下文信息中存在拼音，请将其转换为中文。\n",
    "如果上下文信息中存在英文，请将其翻译为中文。\n",
    "请根据以下上下文回答问题。\n",
    "上下文：{context}\n",
    "问题：{item[\"question\"]}\n",
    "请给出详细准确的回答：\"\"\"\n",
    "            \n",
    "            # 调用API生成回答\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL_NAME,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"你是一个专业的保险问题回答助手。\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "            predicted_answer = response.choices[0].message.content\n",
    "            \n",
    "            # 计算相似度指标\n",
    "            metrics = calculate_similarity_metrics(predicted_answer, item[\"answer\"])\n",
    "            \n",
    "            # 更新总分\n",
    "            for metric in total_metrics:\n",
    "                total_metrics[metric] += metrics[metric]\n",
    "            \n",
    "            results.append({\n",
    "                \"question\": item[\"question\"],\n",
    "                \"predicted_answer\": predicted_answer,\n",
    "                \"reference_answer\": item[\"answer\"],\n",
    "                \"metrics\": metrics,\n",
    "                \"retrieved_documents\": query_result[\"retrieved_documents\"]  # 保存检索到的文档\n",
    "            })\n",
    "            \n",
    "            print(f\"已处理 {len(results)}/{len(qa_data)} 个问题\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"处理问题时出错: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # 计算平均分数\n",
    "    num_questions = len(results)\n",
    "    avg_metrics = {\n",
    "        metric: total_metrics[metric] / num_questions \n",
    "        for metric in total_metrics\n",
    "    }\n",
    "    \n",
    "    # 计算BLEU和ROUGE的平均值\n",
    "    bleu_avg = avg_metrics[\"bleu\"]\n",
    "    rouge_avg = (avg_metrics[\"rouge-1\"] + avg_metrics[\"rouge-2\"] + avg_metrics[\"rouge-l\"]) / 3\n",
    "    \n",
    "    return {\n",
    "        \"bleu_average\": bleu_avg,\n",
    "        \"rouge_average\": rouge_avg,\n",
    "        \"average_metrics\": avg_metrics,\n",
    "        \"results\": results\n",
    "    }\n",
    "\n",
    "\n",
    "# 执行评估\n",
    "print(\"开始评估问答链性能...\")\n",
    "qa_results = evaluate_qa_chain(qa_data, hybrid_results)  # 使用 hybrid_results 替代 retriever\n",
    "\n",
    "# 输出平均分数\n",
    "print(\"\\n问答链评估结果:\")\n",
    "print(f\"BLEU: {qa_results['average_metrics']['bleu']:.4f}\")\n",
    "print(f\"ROUGE-1: {qa_results['average_metrics']['rouge-1']:.4f}\")\n",
    "print(f\"ROUGE-2: {qa_results['average_metrics']['rouge-2']:.4f}\")\n",
    "print(f\"ROUGE-L: {qa_results['average_metrics']['rouge-l']:.4f}\")\n",
    "\n",
    "# 保存结果\n",
    "output_dir = os.path.join(\"..\", \"outputs\", \"outputs_4.0\", \"问答链\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file_path = os.path.join(output_dir, f'qa_chain_results_k=50.json')\n",
    "with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(qa_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n评估完成！结果已保存到: {output_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
