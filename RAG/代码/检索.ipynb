{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7669c106",
   "metadata": {},
   "source": [
    "<font size=100>库+嵌入模型+llm</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b06c7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#配置环境，前面已经配置，这部分不需要重复运行\n",
    "# %%capture --no-stderr\n",
    "# %pip install -U langchain langchain_community pypdf sentence_transformers faiss\n",
    "# %pip install jieba\n",
    "# %pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d5f1f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain                     0.3.25\n",
      "langchain_community           0.3.23\n",
      "pypdf                         5.4.0\n",
      "sentence_transformers         4.1.0\n",
      "device: cpu\n",
      "index.faiss 文件存在\n",
      "向量数据库中数据总数: 759\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库并打印版本信息\n",
    "import langchain, langchain_community, pypdf, sentence_transformers\n",
    "\n",
    "for module in (langchain, langchain_community, pypdf, sentence_transformers):\n",
    "    print(f\"{module.__name__:<30}{module.__version__}\")\n",
    "\n",
    "#导入操作系统和数据处理相关库\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 定义嵌入模型\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import torch\n",
    "from langchain_ollama import OllamaLLM\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'device: {device}')\n",
    "#定义嵌入模型，跑通这部分代码需要开代理\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name='BAAI/bge-large-zh-v1.5',\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "#加载向量数据库\n",
    "# 加载 index.faiss 文件作为向量数据库\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "\n",
    "import faiss\n",
    "import os\n",
    "import numpy as np\n",
    "# 确保路径正确数据库文件\\index.faiss\n",
    "index_path = r\"E:\\RAG\\database\"\n",
    "if os.path.exists(index_path):\n",
    "    print(\"index.faiss 文件存在\")\n",
    "    vector_db = FAISS.load_local(index_path, embedding_model, allow_dangerous_deserialization=True)\n",
    "    \n",
    "# 输出向量数据库vector_db中包含的数据条数\n",
    "print(f\"向量数据库中数据总数: {vector_db.index.ntotal}\")\n",
    "\n",
    "\n",
    "#采用ollama的LLM模型千问\n",
    "llm = OllamaLLM(model=\"qwen:7b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f928d1",
   "metadata": {},
   "source": [
    "<font color=pink size=100>简单检索<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c30ee3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文档 1: L2 距离分数 = 0.8710448741912842, 余弦相似度分数 = 0.5644775734272021\n",
      "文档 2: L2 距离分数 = 0.8904027938842773, 余弦相似度分数 = 0.5547986173049412\n",
      "文档 3: L2 距离分数 = 0.8913600444793701, 余弦相似度分数 = 0.5543199575628979\n",
      "文档 4: L2 距离分数 = 0.8963077068328857, 余弦相似度分数 = 0.551846223802609\n",
      "文档 5: L2 距离分数 = 0.8977255821228027, 余弦相似度分数 = 0.5511373129909685\n",
      "文档 6: L2 距离分数 = 0.9011317491531372, 余弦相似度分数 = 0.5494341331280113\n",
      "文档 7: L2 距离分数 = 0.9016575813293457, 余弦相似度分数 = 0.5491712251149357\n",
      "文档 8: L2 距离分数 = 0.9032906889915466, 余弦相似度分数 = 0.5483546957951592\n",
      "文档 9: L2 距离分数 = 0.9060183763504028, 余弦相似度分数 = 0.5469908216632857\n",
      "文档 10: L2 距离分数 = 0.9064159393310547, 余弦相似度分数 = 0.5467920923993721\n",
      "文档 11: L2 距离分数 = 0.9082849025726318, 余弦相似度分数 = 0.5458575784583303\n",
      "文档 12: L2 距离分数 = 0.9112165570259094, 余弦相似度分数 = 0.5443917320261007\n",
      "文档 13: L2 距离分数 = 0.9124183058738708, 余弦相似度分数 = 0.5437908743352812\n",
      "文档 14: L2 距离分数 = 0.9145948886871338, 余弦相似度分数 = 0.5427025603661471\n",
      "文档 15: L2 距离分数 = 0.9168858528137207, 余弦相似度分数 = 0.5415571073690935\n",
      "文档 16: L2 距离分数 = 0.9215585589408875, 余弦相似度分数 = 0.5392207663358486\n",
      "文档 17: L2 距离分数 = 0.9230747222900391, 余弦相似度分数 = 0.5384626717732199\n",
      "文档 18: L2 距离分数 = 0.924493670463562, 余弦相似度分数 = 0.5377531329507473\n",
      "文档 19: L2 距离分数 = 0.9255205392837524, 余弦相似度分数 = 0.5372397056315028\n",
      "文档 20: L2 距离分数 = 0.92717045545578, 余弦相似度分数 = 0.5364147800086294\n",
      "文档 21: L2 距离分数 = 0.927557647228241, 余弦相似度分数 = 0.5362211867970054\n",
      "文档 22: L2 距离分数 = 0.9276655912399292, 余弦相似度分数 = 0.5361672889187158\n",
      "文档 23: L2 距离分数 = 0.928049623966217, 余弦相似度分数 = 0.5359752613937827\n",
      "文档 24: L2 距离分数 = 0.928199291229248, 余弦相似度分数 = 0.5359003376639895\n",
      "文档 25: L2 距离分数 = 0.9294054508209229, 余弦相似度分数 = 0.535297239476703\n",
      "文档 26: L2 距离分数 = 0.9299590587615967, 余弦相似度分数 = 0.5350205380801188\n",
      "文档 27: L2 距离分数 = 0.9338239431381226, 余弦相似度分数 = 0.5330880853785405\n",
      "文档 28: L2 距离分数 = 0.9339606165885925, 余弦相似度分数 = 0.5330197511044832\n",
      "文档 29: L2 距离分数 = 0.9339643716812134, 余弦相似度分数 = 0.533017851868086\n",
      "文档 30: L2 距离分数 = 0.9365285634994507, 余弦相似度分数 = 0.5317357806925551\n",
      "文档 31: L2 距离分数 = 0.9372397661209106, 余弦相似度分数 = 0.5313801632817458\n",
      "文档 32: L2 距离分数 = 0.9374505877494812, 余弦相似度分数 = 0.5312747510862788\n",
      "文档 33: L2 距离分数 = 0.9387794137001038, 余弦相似度分数 = 0.5306103201109986\n",
      "文档 34: L2 距离分数 = 0.9387975931167603, 余弦相似度分数 = 0.5306012406435746\n",
      "文档 35: L2 距离分数 = 0.9399490356445312, 余弦相似度分数 = 0.5300255464263824\n",
      "文档 36: L2 距离分数 = 0.9400103092193604, 余弦相似度分数 = 0.5299948767947656\n",
      "文档 37: L2 距离分数 = 0.9400764107704163, 余弦相似度分数 = 0.5299617779097923\n",
      "文档 38: L2 距离分数 = 0.9401105046272278, 余弦相似度分数 = 0.5299447422446554\n",
      "文档 39: L2 距离分数 = 0.940122127532959, 余弦相似度分数 = 0.5299390395878503\n",
      "文档 40: L2 距离分数 = 0.9407626390457153, 余弦相似度分数 = 0.5296187117523197\n",
      "文档 41: L2 距离分数 = 0.9416625499725342, 余弦相似度分数 = 0.529168781184634\n",
      "文档 42: L2 距离分数 = 0.9425445199012756, 余弦相似度分数 = 0.5287277226887683\n",
      "文档 43: L2 距离分数 = 0.9426328539848328, 余弦相似度分数 = 0.5286837144017928\n",
      "文档 44: L2 距离分数 = 0.9426440000534058, 余弦相似度分数 = 0.5286779279245296\n",
      "文档 45: L2 距离分数 = 0.9429954290390015, 余弦相似度分数 = 0.5285023154099129\n",
      "文档 46: L2 距离分数 = 0.9438939094543457, 余弦相似度分数 = 0.5280530477865661\n",
      "文档 47: L2 距离分数 = 0.9441512823104858, 余弦相似度分数 = 0.5279244796841688\n",
      "文档 48: L2 距离分数 = 0.9443921446800232, 余弦相似度分数 = 0.5278039431385457\n",
      "文档 49: L2 距离分数 = 0.9444316029548645, 余弦相似度分数 = 0.5277842556135699\n",
      "文档 50: L2 距离分数 = 0.9480801224708557, 余弦相似度分数 = 0.5259599854490081\n",
      "带 L2 距离分数和余弦相似度分数的检索结果已保存到 ../outputs/简单检索/rag_results_with_scores_k=50.txt\n"
     ]
    }
   ],
   "source": [
    "# 在简单的数据检索基础上进行打分操作，采用similarity_search_with_score 方法\n",
    "# 目前最低分数：0.8534546494483948\n",
    "# similarity_search_with_score 方法不仅允许您返回文档，还允许返回查询到查询向量和文档向量之间的距离分数。返回的距离分数是L2距离。因此，分数越低越好。\n",
    "# L2距离分数指的是欧氏距离（Euclidean Distance），也叫L2范数。在向量检索中，L2距离用于衡量两个向量（如查询向量和文档向量）之间的相似度。\n",
    "def rag_search_with_score(query, k):\n",
    "    \"\"\"\n",
    "    使用 vector_db 对输入 query 进行检索，返回前 k 条结果及其相似度分数。\n",
    "    参数:\n",
    "        query (str): 用户输入的检索问题。\n",
    "        k (int): 返回的结果数量，默认为 5。\n",
    "    返回:\n",
    "        list: 每个元素为 dict，包含文档内容、元数据和相似度分数。\n",
    "    \"\"\"\n",
    "    # 调用 vector_db 的 similarity_search_with_score 方法，\n",
    "    # 返回 [(Document, score), ...]，每个元素是文档和对应的相似度分数\n",
    "    results = vector_db.similarity_search_with_score(query, k=k)\n",
    "    scored_docs = []  # 用于存储带分数的检索结果\n",
    "    for doc, score in results:\n",
    "        # 将分数写入文档的元数据，方便后续使用\n",
    "        doc.metadata[\"l2_score\"] = score\n",
    "        # 构造包含内容、元数据和分数的字典，加入结果列表\n",
    "        scored_docs.append({\n",
    "            \"content\": doc.page_content,  # 文档内容\n",
    "            \"metadata\": doc.metadata,     # 文档元数据（包含 l2_score）\n",
    "            \"l2_score\": score            # L2 距离分数\n",
    "        })\n",
    "    return scored_docs  # 返回带分数的检索结果列表\n",
    "\n",
    "# 示例调用\n",
    "query = \"请问保险产品有哪些？\"  # 定义检索问题\n",
    "k = 50  # 设置返回结果数量\n",
    "# 调用函数 rag_search_with_score 进行检索，并获取带分数的结果\n",
    "scored_results = rag_search_with_score(query, k=k)\n",
    "\n",
    "# 基于余弦相似度的检索效果打分\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity_score(query_embedding, doc_embeddings):\n",
    "    \"\"\"\n",
    "    计算查询向量与文档向量之间的余弦相似度。\n",
    "    参数:\n",
    "        query_embedding (np.ndarray): 查询向量。\n",
    "        doc_embeddings (np.ndarray): 文档向量矩阵，每行是一个文档的向量。\n",
    "    返回:\n",
    "        list: 每个文档的余弦相似度分数。\n",
    "    \"\"\"\n",
    "    # 计算余弦相似度\n",
    "    scores = cosine_similarity(query_embedding.reshape(1, -1), doc_embeddings)\n",
    "    return scores.flatten().tolist()\n",
    "\n",
    "# 获取查询向量\n",
    "query_embedding = embedding_model.embed_query(query)\n",
    "\n",
    "# 获取文档向量\n",
    "doc_embeddings = np.array([embedding_model.embed_query(doc['content']) for doc in scored_results])\n",
    "\n",
    "# 计算余弦相似度分数\n",
    "cosine_scores = cosine_similarity_score(np.array(query_embedding), doc_embeddings)\n",
    "\n",
    "# 将余弦相似度分数添加到文档元数据中\n",
    "for doc, score in zip(scored_results, cosine_scores):\n",
    "    doc['metadata']['cosine_score'] = score\n",
    "\n",
    "# 输出每个文档的 L2 距离分数和余弦相似度分数\n",
    "for idx, doc in enumerate(scored_results, 1):\n",
    "    print(f\"文档 {idx}: L2 距离分数 = {doc['metadata']['l2_score']}, 余弦相似度分数 = {doc['metadata']['cosine_score']}\")\n",
    "\n",
    "# 保存带 L2 距离分数和余弦相似度分数的检索结果到文件\n",
    "output_dir = '../outputs/简单检索'\n",
    "os.makedirs(output_dir, exist_ok=True)  # 创建 outputs 目录（如果不存在）\n",
    "with open(f'../outputs/简单检索/rag_results_with_scores_k={k}.txt', 'w', encoding='utf-8') as f:\n",
    "    for idx, doc in enumerate(scored_results, 1):\n",
    "        f.write(f\"\\n===========结果{idx}==========:\\n\")\n",
    "        f.write(f\"内容:\\n{doc['content']}\\n\")\n",
    "        f.write(f\"元数据: {doc['metadata']}\\n\")\n",
    "        f.write(f\"L2 距离分数: {doc['metadata']['l2_score']}\\n\")\n",
    "        f.write(f\"余弦相似度分数: {doc['metadata']['cosine_score']}\\n\")\n",
    "print(f'带 L2 距离分数和余弦相似度分数的检索结果已保存到 ../outputs/简单检索/rag_results_with_scores_k={k}.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dc7623",
   "metadata": {},
   "source": [
    "<font color=pink size=100>上下文压缩</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c4f908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "压缩后的结果已保存到 ../outputs/上下文压缩/compressed_docs_k=20.txt\n"
     ]
    }
   ],
   "source": [
    "#添加上下文压缩使用LLMChainExtractor\n",
    "#现在让我们用 ContextualCompressionRetriever 包装我们的基本检索器。我们将添加一个 LLMChainExtractor，它将迭代最初返回的文档，并从每个文档中提取与查询相关的内容。\n",
    "#问题：运行速度过于慢\n",
    "#优点：效果更佳，减少了冗余信息\n",
    "\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "k = 20  # 设置返回结果数量\n",
    "# 创建上下文压缩检索器\n",
    "compressor_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vector_db.as_retriever(search_kwargs={\"k\": k})\n",
    ")\n",
    "\n",
    "query = \"请问保险产品有哪些？\"\n",
    "compressed_docs = compressor_retriever.invoke(query)\n",
    "\n",
    "\n",
    "# 保存压缩后的结果到 outputs/上下文压缩/compressed_docs_k={k}.txt\n",
    "import os\n",
    "os.makedirs('../outputs/上下文压缩', exist_ok=True)  # 创建中文目录\n",
    "with open(f'../outputs/上下文压缩/compressed_docs_k={k}.txt', 'w', encoding='utf-8') as f:\n",
    "    for idx, doc in enumerate(compressed_docs, 1):\n",
    "        f.write(f\"\\n===========结果{idx}==========:\\n\")\n",
    "        f.write(f\"压缩后的内容:\\n{doc.page_content}\\n\")\n",
    "        f.write(f\"元数据: {doc.metadata}\\n\")\n",
    "print(f'压缩后的结果已保存到 ../outputs/上下文压缩/compressed_docs_k={k}.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aff6728",
   "metadata": {},
   "source": [
    "<font color=pink size=30>生成多个查询</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fb9f54",
   "metadata": {},
   "source": [
    "<font color=blue>没有提示词版本：多查询</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15899dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21860\\2957642352.py:22: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  unique_docs = retriever_from_llm.get_relevant_documents(question)\n",
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. 宏利环球货币保障计划具体是指什么服务？', '2. 这个名为宏利环球货币保障计划的项目，你能详细解释一下吗？', '3. 对于宏利环球货币保障计划这个金融产品，你可以提供一个简化的定义吗？']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索到的文档数量: 6\n",
      "检索结果和拓展问题已保存到 ../outputs/多查询检索\\multi_query_results_with_expanded_questions.txt\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "# 采用 Ollama 的 LLM 模型千问\n",
    "llm = OllamaLLM(model=\"qwen:7b\")\n",
    "\n",
    "# 使用 MultiQueryRetriever 从 LLM 创建多查询检索器\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vector_db.as_retriever(),  # 基于向量数据库的检索器\n",
    "    llm=llm  # 使用的 LLM 模型\n",
    ")\n",
    "\n",
    "# 设置日志记录以查看生成的查询\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "\n",
    "# 定义检索问题\n",
    "question = \"什么是宏利环球货币保障计划？\"\n",
    "\n",
    "# 调用多查询检索器获取相关文档\n",
    "unique_docs = retriever_from_llm.get_relevant_documents(question)\n",
    "\n",
    "# 获取生成的拓展问题\n",
    "expanded_queries = retriever_from_llm.llm_chain.invoke(question)\n",
    "\n",
    "# 输出检索到的文档数量\n",
    "print(f\"检索到的文档数量: {len(unique_docs)}\")\n",
    "\n",
    "# 保存检索结果和拓展问题到文件\n",
    "output_dir = '../outputs/多查询检索'\n",
    "import os\n",
    "os.makedirs(output_dir, exist_ok=True)  # 如果文件夹不存在则创建\n",
    "output_file = os.path.join(output_dir, 'multi_query_results_with_expanded_questions.txt')\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"生成的拓展问题:\\n\")\n",
    "    for idx, query in enumerate(expanded_queries, 1):\n",
    "        f.write(f\"{idx}. {query}\\n\")\n",
    "    f.write(\"\\n检索结果:\\n\")\n",
    "    for idx, doc in enumerate(unique_docs, 1):\n",
    "        f.write(f\"\\n===========结果{idx}==========:\\n\")\n",
    "        f.write(f\"内容: {doc.page_content}\\n\")\n",
    "        f.write(f\"元数据: {doc.metadata}\\n\")\n",
    "\n",
    "# 打印保存结果的路径\n",
    "print(f'检索结果和拓展问题已保存到 {output_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac691f56",
   "metadata": {},
   "source": [
    "<font color=blue>没有提示词版本：多查询+上下文压缩<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cf39ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. 您想了解的保险类型有哪些，以便做出选择？', '2. 除了基础的保险种类，您是否对特定领域的保险产品有所好奇？', '3. 您希望通过购买保险来解决哪些问题或风险？这样我可以帮您找到更匹配的产品。']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索到的文档数量: 10\n",
      "检索结果和拓展问题已保存到 ../outputs/多查询压缩检索\\compressed_docs_with_expanded_questions.txt\n"
     ]
    }
   ],
   "source": [
    "#缺点：速度较慢，可能会导致响应时间延长\n",
    "\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "\n",
    "# 采用 Ollama 的 LLM 模型千问\n",
    "llm = OllamaLLM(model=\"qwen:7b\")\n",
    "\n",
    "# 使用 MultiQueryRetriever 从 LLM 创建多查询检索器\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vector_db.as_retriever(),  # 基于向量数据库的检索器\n",
    "    llm=llm  # 使用的 LLM 模型\n",
    ")\n",
    "\n",
    "\n",
    "#在多查询检索器的基础上添加上下文压缩\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "# 创建上下文压缩检索器\n",
    "compressor_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever_from_llm\n",
    ")\n",
    "\n",
    "\n",
    "# 设置日志记录以查看生成的查询\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "\n",
    "# 调用拟合检索器获取相关文档\n",
    "query = \"请问保险产品有哪些？\"\n",
    "compressed_docs = compressor_retriever.invoke(query)\n",
    "\n",
    "# 获取生成的拓展问题\n",
    "expanded_queries = retriever_from_llm.llm_chain.invoke(query)\n",
    "\n",
    "# 输出检索到的文档数量\n",
    "print(f\"检索到的文档数量: {len(compressed_docs)}\")\n",
    "\n",
    "# 保存检索结果和拓展问题到文件\n",
    "output_dir = '../outputs/多查询压缩检索'\n",
    "import os\n",
    "os.makedirs(output_dir, exist_ok=True)  # 如果文件夹不存在则创建\n",
    "output_file = os.path.join(output_dir, 'compressed_docs_with_expanded_questions1.txt')\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"生成的拓展问题:\\n\")\n",
    "    for idx, query in enumerate(expanded_queries, 1):\n",
    "        f.write(f\"{idx}. {query}\\n\")\n",
    "    f.write(\"\\n检索结果:\\n\")\n",
    "    for idx, doc in enumerate(compressed_docs, 1):\n",
    "        f.write(f\"\\n===========结果{idx}==========:\\n\")\n",
    "        f.write(f\"内容: {doc.page_content}\\n\")\n",
    "        f.write(f\"元数据: {doc.metadata}\\n\")\n",
    "\n",
    "# 打印保存结果的路径\n",
    "print(f'检索结果和拓展问题已保存到 {output_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d83e82",
   "metadata": {},
   "source": [
    "<font color=blue>没有提示词版本：上下文压缩+多查询<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5fc51e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. 您想要了解哪些类型的保险产品？', '2. 如果您是投资者，想知道的是保险产品能带来什么收益吗？', '3. 您需要一份家庭保障计划，还是寻找投资增值的保险产品？']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索到的文档数量: 59\n",
      "检索结果和拓展问题已保存到 ../outputs/多查询压缩检索\\compressed_docs_with_expanded_questions2.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "# 采用 Ollama 的 LLM 模型千问\n",
    "llm = OllamaLLM(model=\"qwen:7b\")\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "#压缩器\n",
    "compressor_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vector_db.as_retriever(search_kwargs={\"k\": k})\n",
    ")\n",
    "# 使用 MultiQueryRetriever 从 LLM 创建多查询检索器\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=compressor_retriever,  # 基于向量数据库的检索器\n",
    "    llm=llm  # 使用的 LLM 模型\n",
    ")\n",
    "\n",
    "# 设置日志记录以查看生成的查询\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "\n",
    "# 定义检索问题\n",
    "question = \"请问保险产品有哪些？\"\n",
    "\n",
    "# 调用多查询检索器获取相关文档\n",
    "unique_docs = retriever_from_llm.get_relevant_documents(question)\n",
    "\n",
    "# 获取生成的拓展问题\n",
    "expanded_queries = retriever_from_llm.llm_chain.invoke(question)\n",
    "\n",
    "# 输出检索到的文档数量\n",
    "print(f\"检索到的文档数量: {len(unique_docs)}\")\n",
    "\n",
    "# 保存检索结果和拓展问题到文件\n",
    "output_dir = '../outputs/多查询压缩检索'\n",
    "import os\n",
    "os.makedirs(output_dir, exist_ok=True)  # 如果文件夹不存在则创建\n",
    "output_file = os.path.join(output_dir, 'compressed_docs_with_expanded_questions2.txt')\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"生成的拓展问题:\\n\")\n",
    "    for idx, query in enumerate(expanded_queries, 1):\n",
    "        f.write(f\"{idx}. {query}\\n\")\n",
    "    f.write(\"\\n检索结果:\\n\")\n",
    "    for idx, doc in enumerate(unique_docs, 1):\n",
    "        f.write(f\"\\n===========结果{idx}==========:\\n\")\n",
    "        f.write(f\" 内容: {doc.page_content}\\n\")\n",
    "        f.write(f\"元数据: {doc.metadata}\\n\")\n",
    "\n",
    "# 打印保存结果的路径\n",
    "print(f'检索结果和拓展问题已保存到 {output_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254207da",
   "metadata": {},
   "source": [
    "<font color=blue>提示词版本：多查询</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fd9f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. 您能简要介绍一下各类保险产品吗？', '2. 我对保险不是很了解，能否给我列举一些常见的保险产品？', '3. 如果我想要购买一份保险，应该从哪些方面去考虑和选择产品？', '4. 您知道哪些保险公司提供的保险品种类丰富吗？可以分享一下吗？', '5. 购买保险时，有哪些因素会左右我选择的产品类型？']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. 您能简要介绍一下各类保险产品吗？', '2. 我对保险不是很了解，能否给我列举一些常见的保险产品？', '3. 如果我想要购买一份保险，应该从哪些方面去考虑和选择产品？', '4. 您知道哪些保险公司提供的保险品种类丰富吗？可以分享一下吗？', '5. 购买保险时，有哪些因素会左右我选择的产品类型？']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索到的文档数量: 17\n",
      "检索结果和拓展问题已保存到 ../outputs/多查询检索_提示词版本\\multi_query_results_with_expanded_questions_prompt_version.txt\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    \"\"\"Output parser for a list of lines.\"\"\"\n",
    "\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return list(filter(None, lines))  # Remove empty lines\n",
    "\n",
    "\n",
    "output_parser = LineListOutputParser()\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "     template=\"\"\"你是一名AI语言模型助手。你的任务是生成五个\n",
    "给定用户问题的不同版本，用于从向量中检索相关文档\n",
    "数据库。通过对用户问题产生多种观点，您的目标是帮助\n",
    "用户克服了基于距离的相似性搜索的一些限制。\n",
    "提供这些替代问题，并用换行符分隔。\n",
    "    Original question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "# Chain\n",
    "llm_chain = QUERY_PROMPT | llm | output_parser\n",
    "\n",
    "# 定义检索问题\n",
    "question = \"请问保险产品有哪些？\"\n",
    "\n",
    "# 创建多查询检索器\n",
    "retriever = MultiQueryRetriever(\n",
    "    retriever=vector_db.as_retriever(), llm_chain=llm_chain, parser_key=\"lines\"\n",
    ")  # \"lines\" is the key (attribute name) of the parsed output\n",
    "\n",
    "# 调用检索器获取相关文档和生成的拓展问题\n",
    "unique_docs = retriever.get_relevant_documents(question)\n",
    "expanded_queries = retriever.llm_chain.invoke(question)\n",
    "\n",
    "# 输出检索到的唯一文档数量\n",
    "\n",
    "print(f\"检索到的文档数量: {len(unique_docs)}\")\n",
    "\n",
    "# 保存检索结果和拓展问题到文件\n",
    "output_dir = '../outputs/多查询检索_提示词版本'\n",
    "import os\n",
    "os.makedirs(output_dir, exist_ok=True)  # 如果文件夹不存在则创建\n",
    "output_file = os.path.join(output_dir, 'multi_query_results_with_expanded_questions_prompt_version.txt')\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"生成的拓展问题:\\n\")\n",
    "    for idx, query in enumerate(expanded_queries, 1):\n",
    "        f.write(f\"{idx}. {query}\\n\")\n",
    "    f.write(\"\\n检索结果:\\n\")\n",
    "    for idx, doc in enumerate(unique_docs, 1):\n",
    "        f.write(f\"\\n===========结果{idx}==========:\\n\")\n",
    "        f.write(f\"内容: {doc.page_content}\\n\")\n",
    "        f.write(f\"元数据: {doc.metadata}\\n\")\n",
    "\n",
    "# 打印保存结果的路径\n",
    "print(f'检索结果和拓展问题已保存到 {output_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e899109",
   "metadata": {},
   "source": [
    "<font color=blue>提示词版本：多查询+上下文压缩<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7474147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. 身故理赔金额是如何依据保单条款决定的？', '2. 计算一份保单的身故赔偿有什么具体步骤？', '3. 如果发生了身故，保单会自动计算并支付多少赔偿金？', '4. 一份保单的身故保险金额如何与投保人的年龄、健康状况等相关？', '5. 被保险人身故后，保险公司如何根据保险条款来确定和支付赔偿金额？']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索到的文档数量: 10\n",
      "检索结果和拓展问题已保存到 ../outputs/多查询检索_提示词版本_压缩检索\\compressed_docs_with_expanded_questions_prompt_version1.txt\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    \"\"\"Output parser for a list of lines.\"\"\"\n",
    "\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return list(filter(None, lines))  # Remove empty lines\n",
    "\n",
    "\n",
    "output_parser = LineListOutputParser()\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "     template=\"\"\"你是一名AI语言模型助手。你的任务是生成五个\n",
    "给定用户问题的不同版本，用于从向量中检索相关文档\n",
    "数据库。通过对用户问题产生多种观点，您的目标是帮助\n",
    "用户克服了基于距离的相似性搜索的一些限制。\n",
    "提供这些替代问题，并用换行符分隔。\n",
    "    Original question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "# Chain\n",
    "llm_chain = QUERY_PROMPT | llm | output_parser\n",
    "\n",
    "# 定义检索问题\n",
    "question = \"什么是宏利环球货币保障计划？\"\n",
    "\n",
    "# 创建多查询检索器\n",
    "retriever_from_llm = MultiQueryRetriever(\n",
    "    retriever=vector_db.as_retriever(), llm_chain=llm_chain, parser_key=\"lines\"\n",
    ")  # \"lines\" is the key (attribute name) of the parsed output\n",
    "\n",
    "# 在多查询检索器的基础上添加上下文压缩\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "k = 20  # 设置返回结果数量\n",
    "# 创建上下文压缩检索器\n",
    "compressor_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever_from_llm\n",
    ")\n",
    "\n",
    "question = \"保单的身故赔偿如何计算?\"\n",
    "\n",
    "# 调用检索器获取相关文档和生成的拓展问题\n",
    "unique_docs =  compressor_retriever.get_relevant_documents(question)\n",
    "expanded_queries = retriever_from_llm.llm_chain.invoke(question)\n",
    "\n",
    "# 输出检索到的唯一文档数量\n",
    "\n",
    "print(f\"检索到的文档数量: {len(unique_docs)}\")\n",
    "\n",
    "# 保存检索结果和拓展问题到文件\n",
    "output_dir = '../outputs/多查询检索_提示词版本_压缩检索'\n",
    "import os\n",
    "os.makedirs(output_dir, exist_ok=True)  # 如果文件夹不存在则创建\n",
    "output_file = os.path.join(output_dir, 'compressed_docs_with_expanded_questions_prompt_version1.txt')\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"生成的拓展问题:\\n\")\n",
    "    for idx, query in enumerate(expanded_queries, 1):\n",
    "        f.write(f\"{idx}. {query}\\n\")\n",
    "    f.write(\"\\n检索结果:\\n\")\n",
    "    for idx, doc in enumerate(unique_docs, 1):\n",
    "        f.write(f\"\\n===========结果{idx}==========:\\n\")\n",
    "        f.write(f\"内容: {doc.page_content}\\n\")\n",
    "        f.write(f\"元数据: {doc.metadata}\\n\")\n",
    "\n",
    "# 打印保存结果的路径\n",
    "print(f'检索结果和拓展问题已保存到 {output_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8ade97",
   "metadata": {},
   "source": [
    "<font color=blue>提示词版本：上下文压缩+多查询<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebc32305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_8924\\359060397.py:48: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  unique_docs = retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 48\u001b[0m\n\u001b[0;32m     43\u001b[0m retriever \u001b[38;5;241m=\u001b[39m MultiQueryRetriever(\n\u001b[0;32m     44\u001b[0m     retriever\u001b[38;5;241m=\u001b[39mcompressor_retriever, llm_chain\u001b[38;5;241m=\u001b[39mllm_chain, parser_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlines\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     45\u001b[0m )  \u001b[38;5;66;03m# \"lines\" is the key (attribute name) of the parsed output\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# 调用检索器获取相关文档和生成的拓展问题\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m unique_docs \u001b[38;5;241m=\u001b[39m retriever\u001b[38;5;241m.\u001b[39mget_relevant_documents(question)\n\u001b[0;32m     49\u001b[0m expanded_queries \u001b[38;5;241m=\u001b[39m retriever\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39minvoke(question)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# 输出检索到的唯一文档数量\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\_api\\deprecation.py:191\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    190\u001b[0m     emit_warning()\n\u001b[1;32m--> 191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\retrievers.py:409\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[1;34m(self, query, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_name:\n\u001b[0;32m    408\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_name\n\u001b[1;32m--> 409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoke(query, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\retrievers.py:258\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    256\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[1;32m--> 258\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28minput\u001b[39m, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs\n\u001b[0;32m    260\u001b[0m     )\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    262\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain\\retrievers\\multi_query.py:172\u001b[0m, in \u001b[0;36mMultiQueryRetriever._get_relevant_documents\u001b[1;34m(self, query, run_manager)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_original:\n\u001b[0;32m    171\u001b[0m     queries\u001b[38;5;241m.\u001b[39mappend(query)\n\u001b[1;32m--> 172\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve_documents(queries, run_manager)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munique_union(documents)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain\\retrievers\\multi_query.py:210\u001b[0m, in \u001b[0;36mMultiQueryRetriever.retrieve_documents\u001b[1;34m(self, queries, run_manager)\u001b[0m\n\u001b[0;32m    208\u001b[0m documents \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries:\n\u001b[1;32m--> 210\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretriever\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m    211\u001b[0m         query, config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_manager\u001b[38;5;241m.\u001b[39mget_child()}\n\u001b[0;32m    212\u001b[0m     )\n\u001b[0;32m    213\u001b[0m     documents\u001b[38;5;241m.\u001b[39mextend(docs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m documents\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\retrievers.py:258\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    256\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[1;32m--> 258\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28minput\u001b[39m, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs\n\u001b[0;32m    260\u001b[0m     )\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    262\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain\\retrievers\\contextual_compression.py:48\u001b[0m, in \u001b[0;36mContextualCompressionRetriever._get_relevant_documents\u001b[1;34m(self, query, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_retriever\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m     45\u001b[0m     query, config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_manager\u001b[38;5;241m.\u001b[39mget_child()}, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m     46\u001b[0m )\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m docs:\n\u001b[1;32m---> 48\u001b[0m     compressed_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_compressor\u001b[38;5;241m.\u001b[39mcompress_documents(\n\u001b[0;32m     49\u001b[0m         docs, query, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child()\n\u001b[0;32m     50\u001b[0m     )\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(compressed_docs)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain\\retrievers\\document_compressors\\chain_extract.py:74\u001b[0m, in \u001b[0;36mLLMChainExtractor.compress_documents\u001b[1;34m(self, documents, query, callbacks)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[0;32m     73\u001b[0m     _input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_input(query, doc)\n\u001b[1;32m---> 74\u001b[0m     output_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39minvoke(_input, config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks})\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain, LLMChain):\n\u001b[0;32m     76\u001b[0m         output \u001b[38;5;241m=\u001b[39m output_[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\runnables\\base.py:3034\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3032\u001b[0m                 \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3033\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3034\u001b[0m                 \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[0;32m   3035\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3036\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\language_models\\llms.py:387\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    384\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    385\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 387\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    388\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    389\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    390\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    391\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    392\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    393\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    394\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    395\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    396\u001b[0m         )\n\u001b[0;32m    397\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    398\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    399\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\language_models\\llms.py:764\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    761\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    762\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    763\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\language_models\\llms.py:971\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    957\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    958\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    959\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    969\u001b[0m         )\n\u001b[0;32m    970\u001b[0m     ]\n\u001b[1;32m--> 971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    972\u001b[0m         prompts,\n\u001b[0;32m    973\u001b[0m         stop,\n\u001b[0;32m    974\u001b[0m         run_managers,\n\u001b[0;32m    975\u001b[0m         new_arg_supported\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(new_arg_supported),\n\u001b[0;32m    976\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    977\u001b[0m     )\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    979\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    980\u001b[0m         callback_managers[idx]\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    981\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[0;32m    989\u001b[0m     ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\language_models\\llms.py:790\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    781\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    787\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    788\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    789\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 790\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    791\u001b[0m                 prompts,\n\u001b[0;32m    792\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    793\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    794\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    795\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    796\u001b[0m             )\n\u001b[0;32m    797\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    798\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    799\u001b[0m         )\n\u001b[0;32m    800\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    801\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_ollama\\llms.py:290\u001b[0m, in \u001b[0;36mOllamaLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    288\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m--> 290\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream_with_aggregation(\n\u001b[0;32m    291\u001b[0m         prompt,\n\u001b[0;32m    292\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    293\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m    294\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    295\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    296\u001b[0m     )\n\u001b[0;32m    297\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_ollama\\llms.py:258\u001b[0m, in \u001b[0;36mOllamaLLM._stream_with_aggregation\u001b[1;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream_with_aggregation\u001b[39m(\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    251\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    256\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GenerationChunk:\n\u001b[0;32m    257\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_generate_stream(prompt, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stream_resp, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    260\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m GenerationChunk(\n\u001b[0;32m    261\u001b[0m                 text\u001b[38;5;241m=\u001b[39mstream_resp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m stream_resp \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    262\u001b[0m                 generation_info\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    263\u001b[0m                     \u001b[38;5;28mdict\u001b[39m(stream_resp) \u001b[38;5;28;01mif\u001b[39;00m stream_resp\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    264\u001b[0m                 ),\n\u001b[0;32m    265\u001b[0m             )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_ollama\\llms.py:213\u001b[0m, in \u001b[0;36mOllamaLLM._create_generate_stream\u001b[1;34m(self, prompt, stop, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_generate_stream\u001b[39m(\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    209\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    210\u001b[0m     stop: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    212\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Union[Mapping[\u001b[38;5;28mstr\u001b[39m, Any], \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_params(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    215\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ollama\\_client.py:170\u001b[0m, in \u001b[0;36mClient._request.<locals>.inner\u001b[1;34m()\u001b[0m\n\u001b[0;32m    167\u001b[0m   e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    168\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_lines():\n\u001b[0;32m    171\u001b[0m   part \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n\u001b[0;32m    172\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m:=\u001b[39m part\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpx\\_models.py:861\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    859\u001b[0m decoder \u001b[38;5;241m=\u001b[39m LineDecoder()\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[1;32m--> 861\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_text():\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m decoder\u001b[38;5;241m.\u001b[39mdecode(text):\n\u001b[0;32m    863\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m line\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpx\\_models.py:848\u001b[0m, in \u001b[0;36mResponse.iter_text\u001b[1;34m(self, chunk_size)\u001b[0m\n\u001b[0;32m    846\u001b[0m chunker \u001b[38;5;241m=\u001b[39m TextChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[0;32m    847\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[1;32m--> 848\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m byte_content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_bytes():\n\u001b[0;32m    849\u001b[0m         text_content \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(byte_content)\n\u001b[0;32m    850\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(text_content):\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpx\\_models.py:829\u001b[0m, in \u001b[0;36mResponse.iter_bytes\u001b[1;34m(self, chunk_size)\u001b[0m\n\u001b[0;32m    827\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[1;32m--> 829\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m raw_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_raw():\n\u001b[0;32m    830\u001b[0m         decoded \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(raw_bytes)\n\u001b[0;32m    831\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(decoded):\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpx\\_models.py:883\u001b[0m, in \u001b[0;36mResponse.iter_raw\u001b[1;34m(self, chunk_size)\u001b[0m\n\u001b[0;32m    880\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[0;32m    882\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[1;32m--> 883\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m raw_stream_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream:\n\u001b[0;32m    884\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_bytes_downloaded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(raw_stream_bytes)\n\u001b[0;32m    885\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(raw_stream_bytes):\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:126\u001b[0m, in \u001b[0;36mBoundSyncStream.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream:\n\u001b[0;32m    127\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpx\\_transports\\default.py:113\u001b[0m, in \u001b[0;36mResponseStream.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 113\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_httpcore_stream:\n\u001b[0;32m    114\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m part\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:361\u001b[0m, in \u001b[0;36mConnectionPoolByteStream.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[1;32m--> 361\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream:\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m part\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:337\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 337\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:329\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_body\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request, kwargs):\n\u001b[1;32m--> 329\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39m_receive_response_body(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    330\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:198\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_body\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    195\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 198\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_event(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mData):\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:212\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    209\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[1;32m--> 212\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREAD_NUM_BYTES, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    214\u001b[0m     )\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpcore\\_backends\\sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[1;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv(max_bytes)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "k=1\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "# 创建上下文压缩检索器\n",
    "compressor_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vector_db.as_retriever(search_kwargs={\"k\": k})\n",
    ")\n",
    "\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    \"\"\"Output parser for a list of lines.\"\"\"\n",
    "\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return list(filter(None, lines))  # Remove empty lines\n",
    "\n",
    "\n",
    "output_parser = LineListOutputParser()\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "     template=\"\"\"你是一名AI语言模型助手。你的任务是生成五个\n",
    "给定用户问题的不同版本，用于从向量中检索相关文档\n",
    "数据库。通过对用户问题产生多种观点，您的目标是帮助\n",
    "用户克服了基于距离的相似性搜索的一些限制。\n",
    "提供这些替代问题，并用换行符分隔。\n",
    "    Original question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "# Chain\n",
    "llm_chain = QUERY_PROMPT | llm | output_parser\n",
    "\n",
    "# 定义检索问题\n",
    "question = \"什么是宏利环球货币保障计划？\"\n",
    "\n",
    "# 创建多查询检索器\n",
    "retriever = MultiQueryRetriever(\n",
    "    retriever=compressor_retriever, llm_chain=llm_chain, parser_key=\"lines\"\n",
    ")  # \"lines\" is the key (attribute name) of the parsed output\n",
    "\n",
    "# 调用检索器获取相关文档和生成的拓展问题\n",
    "unique_docs = retriever.get_relevant_documents(question)\n",
    "expanded_queries = retriever.llm_chain.invoke(question)\n",
    "\n",
    "# 输出检索到的唯一文档数量\n",
    "\n",
    "print(f\"检索到的文档数量: {len(unique_docs)}\")\n",
    "\n",
    "# 保存检索结果和拓展问题到文件\n",
    "output_dir = '../outputs/多查询检索_提示词版本_压缩检索'\n",
    "import os\n",
    "os.makedirs(output_dir, exist_ok=True)  # 如果文件夹不存在则创建\n",
    "output_file = os.path.join(output_dir, 'compressed_docs_with_expanded_questions_prompt_version2.txt')\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"生成的拓展问题:\\n\")\n",
    "    for idx, query in enumerate(expanded_queries, 1):\n",
    "        f.write(f\"{idx}. {query}\\n\")\n",
    "    f.write(\"\\n检索结果:\\n\")\n",
    "    for idx, doc in enumerate(unique_docs, 1):\n",
    "        f.write(f\"\\n===========结果{idx}==========:\\n\")\n",
    "        f.write(f\"内容: {doc.page_content}\\n\")\n",
    "        f.write(f\"元数据: {doc.metadata}\\n\")\n",
    "\n",
    "# 打印保存结果的路径\n",
    "print(f'检索结果和拓展问题已保存到 {output_file}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
