{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50f6aa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13ab2926",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'BAAI/bge-large-zh-v1.5'\n",
    "output_dir = os.path.join('output', f'v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6b81ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "loader=TextLoader(\"D:\\desktop\\code\\data\\宏利環球貨幣保障計劃 保單條款_2022_05.md\", encoding='utf-8')\n",
    "documents=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "516dec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "#配置类 SplitConfig，主要用于文本分割相关的参数配置\n",
    "@dataclass\n",
    "class SplitConfig:\n",
    "    chunk_size: int = 400\n",
    "    chunk_overlap: int = 40\n",
    "    separators: List[str] = ('\\n\\n\\n', '\\n\\n')\n",
    "    force_split: bool = False\n",
    "    output_format: str = 'json' \n",
    "    cache_dir: str = './cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "967593f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
    "\n",
    "##将文本分块结果保存为json\n",
    "def save_chunks_as_json(chunks, filepath):\n",
    "    data = [\n",
    "        {\n",
    "            \"uuid\": chunk.metadata.get('uuid', str(uuid.uuid4())),\n",
    "            \"content\": chunk.page_content,\n",
    "            \"metadata\": chunk.metadata\n",
    "        }\n",
    "        for chunk in chunks\n",
    "    ]\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "##加载分块\n",
    "def load_chunks_from_json(filepath):\n",
    "    from langchain.docstore.document import Document\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    chunks = []\n",
    "    for item in data:\n",
    "        doc = Document(\n",
    "            page_content=item['content'],\n",
    "            metadata=item['metadata']\n",
    "        )\n",
    "        chunks.append(doc)\n",
    "    logging.info(f\"Loaded {len(chunks)} chunks from {filepath}\")\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a5102db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_docs_with_config(documents, config: SplitConfig, cache_name=\"all_docs\"):\n",
    "    os.makedirs(config.cache_dir, exist_ok=True)\n",
    "    filename = f\"split_{config.chunk_size}_{config.chunk_overlap}.{config.output_format}\"\n",
    "    filepath = os.path.join(config.cache_dir, filename)\n",
    "    \n",
    "    if os.path.exists(filepath) and not config.force_split:\n",
    "        logging.info(\"Found existing cache. Loading...\")\n",
    "        if config.output_format == 'json':\n",
    "            return load_chunks_from_json(filepath)\n",
    "    \n",
    "    splitter=MarkdownTextSplitter(\n",
    "        chunk_size=config.chunk_size,\n",
    "        chunk_overlap=config.chunk_overlap\n",
    "    )\n",
    "    chunks = splitter.split_documents(documents)\n",
    "    for chunk in chunks:\n",
    "        chunk.metadata['uuid'] = str(uuid.uuid4())\n",
    "\n",
    "    if config.output_format == 'json':\n",
    "        save_chunks_as_json(chunks, filepath)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dcc49125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded 7 documents from D:/desktop/code/data\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "def load_multiple_documents_from_dir(directory: str, encoding='utf-8') -> List[Document]:\n",
    "    docs = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".md\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            loader = TextLoader(file_path, encoding=encoding)\n",
    "            file_docs = loader.load()\n",
    "            for doc in file_docs:\n",
    "                doc.metadata['source_file'] = filename\n",
    "            docs.extend(file_docs)\n",
    "    logging.info(f\"Loaded {len(docs)} documents from {directory}\")\n",
    "    return docs\n",
    "\n",
    "input_dir = \"D:/desktop/code/data\"  \n",
    "documents = load_multiple_documents_from_dir(input_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fae4c28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Splitting documents with chunk_size=400, overlap=40\n",
      "[INFO] Found existing cache. Loading...\n",
      "[INFO] Loaded 39 chunks from ./cache\\split_400_40.json\n"
     ]
    }
   ],
   "source": [
    "split_config = SplitConfig(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=40,\n",
    "    separators=['\\n\\n\\n', '\\n\\n'],\n",
    "    force_split=False,\n",
    "    output_format='json',\n",
    "    cache_dir='./cache'\n",
    ")\n",
    "logging.info(f\"Splitting documents with chunk_size={split_config.chunk_size}, overlap={split_config.chunk_overlap}\")\n",
    "split_docs = split_docs_with_config(documents, split_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40cff672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Load pretrained SentenceTransformer: BAAI/bge-large-zh-v1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'device: {device}')\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_path,\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d1b5f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xinference.client import Client\n",
    "\n",
    "# # 连接到 Xinference 服务\n",
    "# client = Client(\"http://localhost:9997\")\n",
    "# # 加载嵌入模型\n",
    "# model_uid = client.launch_model(\n",
    "#     model_name=\"bge-large-zh-v1.5\",\n",
    "#     model_size_in_billions=None,  \n",
    "#     quantization=None,\n",
    "#     model_type=\"embedding\"\n",
    "# )\n",
    "\n",
    "# model = client.get_model(model_uid)\n",
    "\n",
    "# # 定义一个自定义的嵌入类，使用 Xinference 模型生成嵌入\n",
    "# class XinferenceEmbeddings:\n",
    "#     def __init__(self, model):\n",
    "#         self.model = model\n",
    "\n",
    "#     def embed_documents(self, texts):\n",
    "#         embeddings = []\n",
    "#         for text in texts:\n",
    "#             result = self.model.create_embedding(input=[text])\n",
    "#             embeddings.append(result[\"data\"][0][\"embedding\"])\n",
    "#         return embeddings\n",
    "\n",
    "#     def embed_query(self, text):\n",
    "#         result = self.model.create_embedding(input=[text])\n",
    "#         return result[\"data\"][0][\"embedding\"]\n",
    "\n",
    "# class CallableXinferenceEmbeddings:\n",
    "#     def __init__(self, xinference_embedder):\n",
    "#         self.embedder = xinference_embedder\n",
    "\n",
    "#     def __call__(self, text):\n",
    "#         return self.embedder.embed_query(text)\n",
    "\n",
    "# # 使用方式\n",
    "# embeddings = CallableXinferenceEmbeddings(XinferenceEmbeddings(model))\n",
    "\n",
    "# # embeddings = XinferenceEmbeddings(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1854d902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_metadata_to_faiss_db(vector_db, metadata_path):\n",
    "    with open(metadata_path, 'rb') as f:\n",
    "        metadatas = pickle.load(f)\n",
    "    for i, doc in enumerate(vector_db.docstore._dict.values()):\n",
    "        doc.metadata = metadatas[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c6584b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_vector_db(docs, store_path, force_rebuild=False):\n",
    "    index_path = os.path.join(store_path, \"faiss_index\")\n",
    "    metadata_path = os.path.join(store_path, \"faiss_metadata.pkl\")\n",
    "\n",
    "    if not os.path.exists(index_path) or not os.path.exists(metadata_path):\n",
    "        force_rebuild = True\n",
    "\n",
    "    if force_rebuild:\n",
    "        os.makedirs(store_path, exist_ok=True)\n",
    "        vector_db = FAISS.from_documents(docs, embedding=embeddings)\n",
    "        vector_db.save_local(index_path)\n",
    "\n",
    "        # 保存 metadatas（Langchain 的 FAISS 默认不会持久化 metadata）\n",
    "        with open(metadata_path, 'wb') as f:\n",
    "            pickle.dump([doc.metadata for doc in docs], f)\n",
    "        \n",
    "    else:\n",
    "        vector_db = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "        attach_metadata_to_faiss_db(vector_db, metadata_path)\n",
    "    return vector_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34f5874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = get_vector_db(split_docs, store_path=os.path.join(output_dir, 'FAISS', 'bge_large_v1.5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad6c892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_faiss_vector_db(new_docs, store_path, embeddings):\n",
    "    \"\"\"\n",
    "    向已有 FAISS 向量数据库中添加新文档并更新持久化索引和 metadata。\n",
    "    \n",
    "    参数：\n",
    "        new_docs (List[Document]): 新文档列表\n",
    "        store_path (str): 向量数据库的持久化路径\n",
    "        embeddings: 已初始化的 embedding 模型（如 HuggingFaceEmbeddings）\n",
    "    \"\"\"\n",
    "    index_path = os.path.join(store_path, \"faiss_index\")\n",
    "    metadata_path = os.path.join(store_path, \"faiss_metadata.pkl\")\n",
    "\n",
    "    # 1. 加载现有向量数据库或初始化新库\n",
    "    if os.path.exists(index_path):\n",
    "        vector_db = FAISS.load_local(index_path, embeddings,allow_dangerous_deserialization=True)\n",
    "    else:\n",
    "        os.makedirs(store_path, exist_ok=True)\n",
    "        vector_db = FAISS.from_documents([], embedding=embeddings)\n",
    "\n",
    "    # 2. 添加新文档并保存\n",
    "    vector_db.add_documents(new_docs)\n",
    "    vector_db.save_local(index_path)\n",
    "\n",
    "    # 3. metadata 更新\n",
    "    if os.path.exists(metadata_path):\n",
    "        with open(metadata_path, 'rb') as f:\n",
    "            metadatas = pickle.load(f)\n",
    "    else:\n",
    "        metadatas = []\n",
    "\n",
    "    metadatas.extend([doc.metadata for doc in new_docs])\n",
    "    with open(metadata_path, 'wb') as f:\n",
    "        pickle.dump(metadatas, f)\n",
    "\n",
    "    print(f\"[INFO] 添加了 {len(new_docs)} 条新文档，向量库已更新并保存。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "13d0d978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 添加了 39 条新文档，向量库已更新并保存。\n"
     ]
    }
   ],
   "source": [
    "vector_db = update_faiss_vector_db(split_docs, store_path=os.path.join(output_dir, 'FAISS', 'bge_large_v1.5'), embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d68cfc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试查询与正确答案\n",
    "test_queries = [\n",
    "    (\"保单持有人何时可行使终期红利锁定权益？\", \"从终期红利锁定周年日（第 15 个保单周年日或其后每个保单周年日 ）起计 31 日内及受保人在世期间，保单持有人可行使终期红利锁定权益\"),\n",
    "    (\"保单货币转换需要满足哪些条件？\", \"保单货币转换申请需满足以下条件：保单持有人必须在货币转换周年日（第 3 个保单周年日或其后每个保单周年日）起计 31 日内递交申请；同一个保单年度内未曾递交过转换申请；申请一旦递交不可撤回或更改；保单的名义金额在转换后必须不少于公司厘定的最低金额；保单持有人必须在公司批准申请前偿还全部欠款\"),\n",
    "]\n",
    "\n",
    "# 对应的正确答案（也可以是文档列表，与你的检索文档匹配）\n",
    "correct_answers = [\n",
    "    \"从终期红利锁定周年日（第 15 个保单周年日或其后每个保单周年日 ）起计 31 日内及受保人在世期间，保单持有人可行使终期红利锁定权益\",\n",
    "    \"保单货币转换申请需满足以下条件：保单持有人必须在货币转换周年日（第 3 个保单周年日或其后每个保单周年日）起计 31 日内递交申请；同一个保单年度内未曾递交过转换申请；申请一旦递交不可撤回或更改；保单的名义金额在转换后必须不少于公司厘定的最低金额；保单持有人必须在公司批准申请前偿还全部欠款\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "78696d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_retrieval_effectiveness(vector_db, test_queries, correct_answers, k=3):\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "\n",
    "    for query, correct_answer in zip(test_queries, correct_answers):\n",
    "        # 对每个查询进行相似性检索\n",
    "        results = vector_db.similarity_search(query[0], k=k)\n",
    "\n",
    "        # 获取检索到的文档文本\n",
    "        retrieved_docs = [doc.page_content for doc in results]\n",
    "\n",
    "        # 计算相关性指标\n",
    "        precision = calculate_precision(retrieved_docs, correct_answer)\n",
    "        recall = calculate_recall(retrieved_docs, correct_answer)\n",
    "\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "\n",
    "    # 输出最终的精确度和召回率\n",
    "    avg_precision = sum(precision_scores) / len(precision_scores)\n",
    "    avg_recall = sum(recall_scores) / len(recall_scores)\n",
    "\n",
    "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "\n",
    "def calculate_precision(retrieved_docs, correct_answer):\n",
    "    # 判断检索文档中与正确答案的匹配程度（可以设定阈值，简单地判断字符串匹配）\n",
    "    return int(any(correct_answer in doc for doc in retrieved_docs))\n",
    "\n",
    "def calculate_recall(retrieved_docs, correct_answer):\n",
    "    # 计算召回率：检索到的正确答案是否出现在返回的文档中\n",
    "    return int(any(correct_answer in doc for doc in retrieved_docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ce08975e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(retrieved_docs, correct_answer):\n",
    "    relevant_count = 0\n",
    "    for doc in retrieved_docs:\n",
    "        for keyword in correct_answer.split():\n",
    "            if keyword in doc:\n",
    "                relevant_count += 1\n",
    "                break\n",
    "    return relevant_count / len(retrieved_docs) if len(retrieved_docs) > 0 else 0\n",
    "\n",
    "def calculate_recall(retrieved_docs, correct_answer):\n",
    "    # 标记是否找到相关文档\n",
    "    found_relevant = False\n",
    "    for doc in retrieved_docs:\n",
    "        for keyword in correct_answer.split():\n",
    "            if keyword in doc:\n",
    "                found_relevant = True\n",
    "                break\n",
    "        if found_relevant:\n",
    "            break\n",
    "    return 1 if found_relevant else 0\n",
    "\n",
    "def test_retrieval_effectiveness(vector_db, test_queries, correct_answers, k=3):\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "\n",
    "    for query, correct_answer in zip(test_queries, correct_answers):\n",
    "        results = vector_db.similarity_search(query[0], k=k)\n",
    "        print(f\"Query: {query[0]}\")\n",
    "        print(f\"Correct Answer: {correct_answer}\")\n",
    "        print(f\"Retrieved Documents: {[doc.page_content[:100] for doc in results]}\")\n",
    "        retrieved_docs = [doc.page_content for doc in results]\n",
    "        precision = calculate_precision(retrieved_docs, correct_answer)\n",
    "        recall = calculate_recall(retrieved_docs, correct_answer)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "    avg_precision = sum(precision_scores) / len(precision_scores)\n",
    "    avg_recall = sum(recall_scores) / len(recall_scores)\n",
    "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average Recall: {avg_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "93909615",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'similarity_search'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 执行测试\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtest_retrieval_effectiveness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector_db\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_queries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrect_answers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[46], line 27\u001b[0m, in \u001b[0;36mtest_retrieval_effectiveness\u001b[1;34m(vector_db, test_queries, correct_answers, k)\u001b[0m\n\u001b[0;32m     24\u001b[0m recall_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query, correct_answer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(test_queries, correct_answers):\n\u001b[1;32m---> 27\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mvector_db\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search\u001b[49m(query[\u001b[38;5;241m0\u001b[39m], k\u001b[38;5;241m=\u001b[39mk)\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrect Answer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect_answer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'similarity_search'"
     ]
    }
   ],
   "source": [
    "# 执行测试\n",
    "test_retrieval_effectiveness(vector_db, test_queries, correct_answers, k=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
