{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbdb470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "# === 火山引擎 API 设置 ===\n",
    "API_KEY = \"2b064694-cb3b-49e3-bb6e-55ef9ad8351f\"\n",
    "API_URL = \"https://ark.cn-beijing.volces.com/api/v3\"  # 请替换为你所使用的大模型接口\n",
    "\n",
    "HEADERS = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\"\n",
    "}\n",
    "\n",
    "# === 加载检索结果（来自上一步的 JSON 文件）===\n",
    "with open(\"../retrieval_result/QA_ds/embedding_0.7(20).json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    retrieval_results = json.load(f)[\"results\"]\n",
    "\n",
    "# === 构建 Prompt（模板）===\n",
    "def build_prompt(query, documents, max_docs=3):\n",
    "    context = \"\"\n",
    "    for i, doc in enumerate(documents[:max_docs]):\n",
    "        content = doc[\"content\"].strip().replace(\"\\n\", \" \")\n",
    "        metadata = doc[\"metadata\"]\n",
    "        context += f\"[文档{i+1} - 来源: {metadata.get('source_file')}, 页码: {metadata.get('page_num')}]:\\n{content}\\n\\n\"\n",
    "    \n",
    "    prompt = f\"\"\"你是一名保险知识问答助手，请基于以下参考资料回答用户的问题。\n",
    "\n",
    "【参考资料】\n",
    "{context}\n",
    "\n",
    "【问题】\n",
    "{query}\n",
    "\n",
    "【回答】\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "# === 请求火山引擎大模型 API ===\n",
    "def call_model(prompt, temperature=0.3):\n",
    "    payload = {\n",
    "        \"model\": \"deepseek-r1-distill-qwen-32b-250120\",  # 替换为你实际使用的模型，比如 glm-4、baichuan2-13b-chat、qwen-max 等\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": 0.9,\n",
    "        \"max_tokens\": 1024\n",
    "    }\n",
    "\n",
    "    response = requests.post(API_URL, headers=HEADERS, data=json.dumps(payload))\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    else:\n",
    "        print(f\"API Error: {response.status_code}, {response.text}\")\n",
    "        return \"生成失败\"\n",
    "\n",
    "# === 主流程：生成回答并保存 ===\n",
    "output_results = []\n",
    "\n",
    "for item in tqdm(retrieval_results, desc=\"生成中\"):\n",
    "    query = item[\"query\"]\n",
    "    correct_answer = item[\"correct_answer\"]\n",
    "    retrieved_docs = item[\"retrieved_documents\"]\n",
    "\n",
    "    prompt = build_prompt(query, retrieved_docs)\n",
    "    generated_answer = call_model(prompt)\n",
    "\n",
    "    output_results.append({\n",
    "        \"query\": query,\n",
    "        \"reference_answer\": correct_answer,\n",
    "        \"generated_answer\": generated_answer,\n",
    "        \"source_docs\": retrieved_docs\n",
    "    })\n",
    "\n",
    "# === 保存生成结果 ===\n",
    "output_path = \"./generation_result/QA_ds/generation_doubao.json\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"生成完成，保存至 {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7146a5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成中: 100%|██████████| 222/222 [17:41<00:00,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成完成，保存至 ./generation_result/QA_ds/generation_doubao.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# === CHAT_MODEL 类封装 ===\n",
    "class CHAT_MODEL:\n",
    "    def __init__(self, api_key, base_url, model_name):\n",
    "        self.llm = OpenAI(\n",
    "            api_key=api_key,\n",
    "            base_url=base_url\n",
    "        )\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def chat(self, user_prompt):\n",
    "        completion = self.llm.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "        )\n",
    "        response = completion.choices[0].message.content\n",
    "        return response\n",
    "\n",
    "\n",
    "# === 构建 Prompt（模板）===\n",
    "def build_prompt(query, documents, max_docs=3):\n",
    "    context = \"\"\n",
    "    for i, doc in enumerate(documents[:max_docs]):\n",
    "        content = doc[\"content\"].strip().replace(\"\\n\", \" \")\n",
    "        metadata = doc.get(\"metadata\", {})\n",
    "        context += f\"[文档{i+1} - 来源: {metadata.get('source_file', '未知')}, 页码: {metadata.get('page_num', '未知')}]:\\n{content}\\n\\n\"\n",
    "\n",
    "    prompt = f\"\"\"你是一名保险知识问答助手，请基于以下参考资料回答用户的问题。\n",
    "\n",
    "【参考资料】\n",
    "{context}\n",
    "\n",
    "【问题】\n",
    "{query}\n",
    "\n",
    "【回答】\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def main():\n",
    "    # === 配置 ===\n",
    "    api_key = \"2b064694-cb3b-49e3-bb6e-55ef9ad8351f\"  # 你的 API Key\n",
    "    base_url = \"https://ark.cn-beijing.volces.com/api/v3\"  # 大模型接口地址，示例火山引擎\n",
    "    model_name = \"deepseek-r1-distill-qwen-32b-250120\"  # 你要调用的模型名\n",
    "\n",
    "    chat_model = CHAT_MODEL(api_key=api_key, base_url=base_url, model_name=model_name)\n",
    "\n",
    "    # === 读取检索结果 ===\n",
    "    retrieval_json_path = \"../retrieval_result/QA_ds/embedding_0.7(20).json\"\n",
    "    with open(retrieval_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        retrieval_results = json.load(f)[\"results\"]\n",
    "\n",
    "    output_results = []\n",
    "\n",
    "    # === 遍历检索结果，构建 prompt 并调用模型生成回答 ===\n",
    "    for item in tqdm(retrieval_results, desc=\"生成中\"):\n",
    "        query = item.get(\"query\", \"\")\n",
    "        correct_answer = item.get(\"correct_answer\", \"\")\n",
    "        retrieved_docs = item.get(\"retrieved_documents\", [])\n",
    "\n",
    "        prompt = build_prompt(query, retrieved_docs)\n",
    "        generated_answer = chat_model.chat(prompt)\n",
    "\n",
    "        output_results.append({\n",
    "            \"query\": query,\n",
    "            \"reference_answer\": correct_answer,\n",
    "            \"generated_answer\": generated_answer,\n",
    "            \"source_docs\": retrieved_docs\n",
    "        })\n",
    "\n",
    "    # === 保存生成结果 ===\n",
    "    output_path = \"./generation_result/QA_ds/generation_doubao.json\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"生成完成，保存至 {output_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfc33469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\36325\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Generating:   0%|          | 0/222 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache D:\\Temp\\jieba.cache\n",
      "Loading model cost 0.358 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Generating: 100%|██████████| 222/222 [21:35<00:00,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成完成，保存至 ./generation_result/QA_ds/generation_ds2.json\n",
      "\n",
      "平均指标：\n",
      "bleu: 0.0715\n",
      "rouge_1_f: 0.2520\n",
      "rouge_L_f: 0.2481\n",
      "生成完成，保存至 ./generation_result/QA_ds/generation_ds2.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import jieba \n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import nltk\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "class CHAT_MODEL:\n",
    "    def __init__(self, api_key, base_url, model_name):\n",
    "        self.llm = OpenAI(\n",
    "            api_key=api_key,\n",
    "            base_url=base_url\n",
    "        )\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def chat(self, user_prompt):\n",
    "        completion = self.llm.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "        )\n",
    "        response = completion.choices[0].message.content\n",
    "        return response\n",
    "\n",
    "\n",
    "def build_prompt(query, documents, max_docs=3):\n",
    "    context = \"\"\n",
    "    for i, doc in enumerate(documents[:max_docs]):\n",
    "        content = doc[\"content\"].strip().replace(\"\\n\", \" \")\n",
    "        metadata = doc.get(\"metadata\", {})\n",
    "        context += f\"[文档{i + 1} - 来源: {metadata.get('source_file', '未知')}, 页码: {metadata.get('page_num', '未知')}]:\\n{content}\\n\\n\"\n",
    "\n",
    "    prompt = f\"\"\"你是一名保险知识问答助手，请基于以下参考资料回答用户的问题。\n",
    "\n",
    "【参考资料】\n",
    "{context}\n",
    "\n",
    "【问题】\n",
    "{query}\n",
    "\n",
    "【回答】\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def compute_metrics(generated, reference, retrieved_docs):\n",
    "    # BLEU 计算\n",
    "    # reference_tokens = [nltk.word_tokenize(reference)] if reference else [[]]\n",
    "    # generated_tokens = nltk.word_tokenize(generated) if generated else []\n",
    "    # try:\n",
    "    #     bleu = nltk.translate.bleu_score.sentence_bleu(reference_tokens, generated_tokens)\n",
    "    # except:\n",
    "    #     bleu = 0.0\n",
    "    reference_tokens = [list(jieba.cut(reference))] if reference else [[]]\n",
    "    generated_tokens = list(jieba.cut(generated)) if generated else []\n",
    "    try:\n",
    "        bleu = sentence_bleu(\n",
    "            reference_tokens,\n",
    "            generated_tokens,\n",
    "            smoothing_function=SmoothingFunction().method1\n",
    "        )\n",
    "    except ZeroDivisionError as e:\n",
    "        print(f\"BLEU 计算错误: {e}\")\n",
    "        bleu = 0.0\n",
    "\n",
    "    # ROUGE 计算\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = scorer.score(reference or \"\", generated or \"\")\n",
    "    rouge_1_f = rouge_scores['rouge1'].fmeasure\n",
    "    rouge_L_f = rouge_scores['rougeL'].fmeasure\n",
    "\n",
    "    return bleu, rouge_1_f, rouge_L_f\n",
    "\n",
    "\n",
    "def main():\n",
    "    api_key = \"2b064694-cb3b-49e3-bb6e-55ef9ad8351f\"  # 你的 API Key\n",
    "    base_url = \"https://ark.cn-beijing.volces.com/api/v3\"\n",
    "    model_name = \"deepseek-r1-distill-qwen-7b-250120\"\n",
    "    chat_model = CHAT_MODEL(api_key=api_key, base_url=base_url, model_name=model_name)\n",
    "\n",
    "    retrieval_json_path = \"../retrieval_result/QA_ds/embedding_0.7(20).json\"\n",
    "    with open(retrieval_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        retrieval_results = json.load(f)[\"results\"]\n",
    "\n",
    "    output_results = []\n",
    "\n",
    "    for item in tqdm(retrieval_results, desc=\"Generating\"):\n",
    "        query = item.get(\"query\", \"\")\n",
    "        correct_answer = item.get(\"correct_answer\", \"\")\n",
    "        retrieved_docs = item.get(\"retrieved_documents\", [])\n",
    "\n",
    "        prompt = build_prompt(query, retrieved_docs)\n",
    "        generated_answer = chat_model.chat(prompt)\n",
    "\n",
    "        bleu, rouge_1_f, rouge_L_f = compute_metrics(generated_answer, correct_answer, retrieved_docs)\n",
    "        \n",
    "        output_results.append({\n",
    "            \"query\": query,\n",
    "            \"reference_answer\": correct_answer,\n",
    "            \"generated_answer\": generated_answer,\n",
    "            \"source_docs\": retrieved_docs,\n",
    "            \"metrics\": {\n",
    "                \"bleu\": bleu,\n",
    "                \"rouge_1_f\": rouge_1_f,\n",
    "                \"rouge_L_f\": rouge_L_f\n",
    "            }\n",
    "        })\n",
    "\n",
    "    output_path = \"./generation_result/QA_ds/generation_ds2.json\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "        # 计算平均指标\n",
    "    total = len(output_results)\n",
    "    avg_bleu = sum(item[\"metrics\"][\"bleu\"] for item in output_results) / total\n",
    "    avg_rouge1 = sum(item[\"metrics\"][\"rouge_1_f\"] for item in output_results) / total\n",
    "    avg_rougeL = sum(item[\"metrics\"][\"rouge_L_f\"] for item in output_results) / total\n",
    "\n",
    "    average_metrics = {\n",
    "        \"bleu\": avg_bleu,\n",
    "        \"rouge_1_f\": avg_rouge1,\n",
    "        \"rouge_L_f\": avg_rougeL\n",
    "    }\n",
    "\n",
    "    final_output = {\n",
    "        \"average_metrics\": average_metrics,\n",
    "        \"results\": output_results\n",
    "    }\n",
    "\n",
    "    # 写入文件\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_output, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # 控制台打印\n",
    "    print(f\"生成完成，保存至 {output_path}\")\n",
    "    print(\"\\n平均指标：\")\n",
    "    for k, v in average_metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "\n",
    "    print(f\"生成完成，保存至 {output_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e2fe78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 222/222 [24:35<00:00,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成完成，保存至 ./generation_result/QA_ds/generation_ds2.json\n",
      "\n",
      "平均指标：\n",
      "bleu: 0.0714\n",
      "rouge_1_f: 0.2478\n",
      "rouge_L_f: 0.2419\n",
      "bert_score_f1: 0.3807\n",
      "生成完成，保存至 ./generation_result/QA_ds/generation_ds2.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import nltk\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import jieba\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "\n",
    "class CHAT_MODEL:\n",
    "    def __init__(self, api_key, base_url, model_name):\n",
    "        self.llm = OpenAI(\n",
    "            api_key=api_key,\n",
    "            base_url=base_url\n",
    "        )\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def chat(self, user_prompt):\n",
    "        completion = self.llm.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "        )\n",
    "        response = completion.choices[0].message.content\n",
    "        return response\n",
    "\n",
    "\n",
    "def build_prompt(query, documents, max_docs=3):\n",
    "    context = \"\"\n",
    "    for i, doc in enumerate(documents[:max_docs]):\n",
    "        content = doc[\"content\"].strip().replace(\"\\n\", \" \")\n",
    "        metadata = doc.get(\"metadata\", {})\n",
    "        context += f\"[文档{i + 1} - 来源: {metadata.get('source_file', '未知')}, 页码: {metadata.get('page_num', '未知')}]:\\n{content}\\n\\n\"\n",
    "\n",
    "    prompt = f\"\"\"你是一名保险知识问答助手，请基于以下参考资料回答用户的问题。\n",
    "\n",
    "【参考资料】\n",
    "{context}\n",
    "\n",
    "【问题】\n",
    "{query}\n",
    "\n",
    "【回答】\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "\n",
    "# 初始化 ROUGE scorer\n",
    "scorer_word = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "scorer_char = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=False)\n",
    "\n",
    "def compute_metrics(generated, reference):\n",
    "    # 空文本处理\n",
    "    if not reference or not generated:\n",
    "        return {\n",
    "            \"bleu\": 0.0,\n",
    "            \"rouge_1_f\": 0.0,\n",
    "            \"rouge_L_f\": 0.0,\n",
    "            \"bert_score_f1\": 0.0\n",
    "        }\n",
    "\n",
    "    # -------------------------------\n",
    "    # 1. BLEU (jieba分词 + 平滑)\n",
    "    reference_tokens = [list(jieba.cut(reference))]\n",
    "    generated_tokens = list(jieba.cut(generated))\n",
    "    try:\n",
    "        bleu = sentence_bleu(\n",
    "            reference_tokens,\n",
    "            generated_tokens,\n",
    "            smoothing_function=SmoothingFunction().method1\n",
    "        )\n",
    "    except ZeroDivisionError as e:\n",
    "        print(f\"BLEU 计算错误: {e}\")\n",
    "        bleu = 0.0\n",
    "\n",
    "    # -------------------------------\n",
    "    # 2. ROUGE (字级 + 词级 多粒度平均)\n",
    "    # 字级（不切词）\n",
    "    rouge_char = scorer_char.score(reference, generated)\n",
    "    rouge_char_1_f = rouge_char['rouge1'].fmeasure\n",
    "    rouge_char_L_f = rouge_char['rougeL'].fmeasure\n",
    "\n",
    "    # 词级（jieba切词）\n",
    "    ref_word = \" \".join(jieba.cut(reference))\n",
    "    gen_word = \" \".join(jieba.cut(generated))\n",
    "    rouge_word = scorer_word.score(ref_word, gen_word)\n",
    "    rouge_word_1_f = rouge_word['rouge1'].fmeasure\n",
    "    rouge_word_L_f = rouge_word['rougeL'].fmeasure\n",
    "\n",
    "    # 多粒度融合（平均）\n",
    "    rouge_1_f = (rouge_char_1_f + rouge_word_1_f) / 2\n",
    "    rouge_L_f = (rouge_char_L_f + rouge_word_L_f) / 2\n",
    "\n",
    "    # -------------------------------\n",
    "    # 3. BERTScore (基于语义)\n",
    "    P, R, F1 = bert_score([generated], [reference], lang='zh', rescale_with_baseline=True)\n",
    "    bert_f1 = F1[0].item()\n",
    "\n",
    "    # -------------------------------\n",
    "    return bleu, rouge_1_f, rouge_L_f, bert_f1\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    api_key = \"2b064694-cb3b-49e3-bb6e-55ef9ad8351f\"\n",
    "    base_url = \"https://ark.cn-beijing.volces.com/api/v3\"\n",
    "    model_name = \"deepseek-r1-distill-qwen-7b-250120\"\n",
    "    chat_model = CHAT_MODEL(api_key=api_key, base_url=base_url, model_name=model_name)\n",
    "\n",
    "    retrieval_json_path = \"../retrieval_result/QA_ds/embedding_0.7(20).json\"\n",
    "    with open(retrieval_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        retrieval_results = json.load(f)[\"results\"]\n",
    "\n",
    "    output_results = []\n",
    "\n",
    "    for item in tqdm(retrieval_results, desc=\"Generating\"):\n",
    "        query = item.get(\"query\", \"\")\n",
    "        correct_answer = item.get(\"correct_answer\", \"\")\n",
    "        retrieved_docs = item.get(\"retrieved_documents\", [])\n",
    "\n",
    "        prompt = build_prompt(query, retrieved_docs)\n",
    "        generated_answer = chat_model.chat(prompt)\n",
    "\n",
    "        bleu, rouge_1_f, rouge_L_f, bert_score_f1 = compute_metrics(generated_answer, correct_answer)\n",
    "\n",
    "        output_results.append({\n",
    "            \"query\": query,\n",
    "            \"reference_answer\": correct_answer,\n",
    "            \"generated_answer\": generated_answer,\n",
    "            \"source_docs\": retrieved_docs,\n",
    "            \"metrics\": {\n",
    "                \"bleu\": bleu,\n",
    "                \"rouge_1_f\": rouge_1_f,\n",
    "                \"rouge_L_f\": rouge_L_f,\n",
    "                \"bert_score_f1\": bert_score_f1  \n",
    "            }\n",
    "        })\n",
    "\n",
    "    output_path = \"./generation_result/QA_ds/generation_ds2.json\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "        # 计算平均指标\n",
    "    total = len(output_results)\n",
    "    avg_bleu = sum(item[\"metrics\"][\"bleu\"] for item in output_results) / total\n",
    "    avg_rouge1 = sum(item[\"metrics\"][\"rouge_1_f\"] for item in output_results) / total\n",
    "    avg_rougeL = sum(item[\"metrics\"][\"rouge_L_f\"] for item in output_results) / total\n",
    "    avg_bert = sum(item[\"metrics\"][\"bert_score_f1\"] for item in output_results) / total\n",
    "\n",
    "    average_metrics = {\n",
    "        \"bleu\": avg_bleu,\n",
    "        \"rouge_1_f\": avg_rouge1,\n",
    "        \"rouge_L_f\": avg_rougeL,\n",
    "        \"bert_score_f1\": avg_bert\n",
    "    }\n",
    "\n",
    "    final_output = {\n",
    "        \"average_metrics\": average_metrics,\n",
    "        \"results\": output_results\n",
    "    }\n",
    "\n",
    "    # 写入文件\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_output, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # 控制台打印\n",
    "    print(f\"生成完成，保存至 {output_path}\")\n",
    "    print(\"\\n平均指标：\")\n",
    "    for k, v in average_metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "\n",
    "    print(f\"生成完成，保存至 {output_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
