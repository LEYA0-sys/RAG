{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d665c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all documents from vector_db...\n",
      "Total documents loaded: 1038\n",
      "Duplicate & metadata report saved to dedup_report.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from langchain.vectorstores import FAISS  \n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name='BAAI/bge-large-zh-v1.5',\n",
    "    model_kwargs={'device':'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "vector_db = FAISS.load_local(\n",
    "    r\"output\\v1\\FAISS\\bge_large_v1.5\\faiss_index\",\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True  \n",
    ")\n",
    "def detect_duplicates_and_missing_metadata(vector_db, export_json=\"dedup_report.json\", export_excel=\"dedup_report.xlsx\"):\n",
    "    # 获取所有文档\n",
    "    print(\"Loading all documents from vector_db...\")\n",
    "    all_docs = list(vector_db.docstore._dict.values())  \n",
    "\n",
    "    print(f\"Total documents loaded: {len(all_docs)}\")\n",
    "\n",
    "    # 映射：content -> [uuid, source_file]\n",
    "    content_to_docs = defaultdict(list)\n",
    "    unknown_source_docs = []\n",
    "\n",
    "    for doc in all_docs:\n",
    "        content = doc.page_content.strip()\n",
    "        uuid = doc.metadata.get(\"uuid\", \"unknown\")\n",
    "        source_file = doc.metadata.get(\"source_file\", \"unknown\")\n",
    "\n",
    "        content_to_docs[content].append({\n",
    "            \"uuid\": uuid,\n",
    "            \"source_file\": source_file\n",
    "        })\n",
    "\n",
    "        # 记录 source_file 缺失的\n",
    "        if source_file == \"unknown\":\n",
    "            unknown_source_docs.append({\n",
    "                \"uuid\": uuid,\n",
    "                \"content\": content[:100] + \"...\" if len(content) > 100 else content\n",
    "            })\n",
    "\n",
    "    # 找出重复内容的文档（content 对应多个 uuid）\n",
    "    duplicate_content_docs = []\n",
    "    for content, docs in content_to_docs.items():\n",
    "        if len(docs) > 1:\n",
    "            duplicate_content_docs.append({\n",
    "                \"content_sample\": content[:100] + \"...\" if len(content) > 100 else content,\n",
    "                \"matched_docs\": docs\n",
    "            })\n",
    "\n",
    "    # 结果汇总\n",
    "    report = {\n",
    "        \"total_documents\": len(all_docs),\n",
    "        \"duplicates_found\": len(duplicate_content_docs),\n",
    "        \"unknown_source_files\": len(unknown_source_docs),\n",
    "        \"duplicate_content_docs\": duplicate_content_docs,\n",
    "        \"unknown_source_docs\": unknown_source_docs\n",
    "    }\n",
    "\n",
    "    # 导出 JSON 文件\n",
    "    with open(export_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Duplicate & metadata report saved to {export_json}\")\n",
    "\n",
    "\n",
    "detect_duplicates_and_missing_metadata(vector_db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de57a218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Temp\\ipykernel_10644\\4060222751.py:8: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceBgeEmbeddings(\n",
      "d:\\naconda\\envs\\ocr\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "导出TOP20: 100%|██████████| 97/97 [00:12<00:00,  7.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "导出完成：./manual_check/sample_top20_export.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "# 初始化\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name='BAAI/bge-large-zh-v1.5',\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "vector_db = FAISS.load_local(\n",
    "    r\"output\\v1\\FAISS\\bge_large_v1.5\\faiss_index\",\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# 加载测试数据\n",
    "with open(r'D:\\desktop\\code\\QA\\test.json', 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "test_queries = [item[\"question\"] for item in test_data]\n",
    "correct_answers = [item[\"answer\"] for item in test_data]\n",
    "\n",
    "# 导出前20条 query，top20 个文档\n",
    "export_data = []\n",
    "\n",
    "for query, correct_answer in tqdm(zip(test_queries, correct_answers), total=len(test_queries), desc=\"导出TOP20\"):\n",
    "    results = vector_db.similarity_search(query, k=20)\n",
    "    export_data.append({\n",
    "        \"query\": query,\n",
    "        \"correct_answer\": correct_answer,\n",
    "        \"retrieved_documents\": [\n",
    "            {\"rank\": i+1, \"content\": doc.page_content}\n",
    "            for i, doc in enumerate(results)\n",
    "        ]\n",
    "    })\n",
    "\n",
    "# 保存\n",
    "os.makedirs(\"./manual_check\", exist_ok=True)\n",
    "export_file = \"./manual_check/sample_top20_export.json\"\n",
    "with open(export_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(export_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"导出完成：{export_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
